{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":829,"status":"ok","timestamp":1646666236322,"user":{"displayName":"吴睿天","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyHoWC83255DLzqGIvhduYRpRsVBg62ekSrWVV=s64","userId":"16191779364250004927"},"user_tz":-480},"id":"WsBkc_dbAcfh","outputId":"64bdda61-f5d0-4684-c1b5-d4e0c15cad50"},"outputs":[],"source":["import torch\n","\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import numpy as np\n","import os\n","from PIL import Image\n","from sklearn.utils import class_weight\n","from datasets import list_metrics, load_metric\n","from transformers import BertTokenizer, VisualBertForPreTraining, AutoTokenizer\n","from transformers import Swinv2Model,AutoImageProcessor"]},{"cell_type":"markdown","metadata":{},"source":["## Load in Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EN8lH9xXSvs5"},"outputs":[],"source":["path = \"/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/data/\"\n","df = pd.read_csv(path + 'data.csv')\n","df_train = df[:8500]\n","df_val = df[8500:9540]\n","df_test = df[9540:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1646666236850,"user":{"displayName":"吴睿天","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyHoWC83255DLzqGIvhduYRpRsVBg62ekSrWVV=s64","userId":"16191779364250004927"},"user_tz":-480},"id":"a_192_dTP5Sn","outputId":"68976e7c-0fd6-41c9-add1-5a42ea2999f3"},"outputs":[],"source":["df_train['text_len'] = df_train['text'].str.split().str.len()\n","df_train['idx'] = df_train['id'].astype(str).str.zfill(5)\n","df_val['idx'] = df_val['id'].astype(str).str.zfill(5)\n","df_test['idx'] = df_test['id'].astype(str).str.zfill(5)"]},{"cell_type":"markdown","metadata":{"id":"8_ieiNwrLUzt"},"source":["## Compute Class Weight"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":371,"status":"ok","timestamp":1646666237195,"user":{"displayName":"吴睿天","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyHoWC83255DLzqGIvhduYRpRsVBg62ekSrWVV=s64","userId":"16191779364250004927"},"user_tz":-480},"id":"HjGXoyRtLXuY","outputId":"56e581c6-6aaa-4207-bb1f-c44559d53410"},"outputs":[],"source":["\n","y_train = df_train[\"label\"].values.tolist()\n","class_weights = class_weight.compute_class_weight(class_weight ='balanced',\n","                                                 classes = np.unique(y_train),\n","                                                 y = y_train)\n","print(class_weights)"]},{"cell_type":"markdown","metadata":{"id":"BrmDPEtUe7Hm"},"source":["## Load Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":482,"status":"ok","timestamp":1646666237671,"user":{"displayName":"吴睿天","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyHoWC83255DLzqGIvhduYRpRsVBg62ekSrWVV=s64","userId":"16191779364250004927"},"user_tz":-480},"id":"15HVpWiKPSkH","outputId":"8fab97dd-1941-4854-b462-61dbb7734a83"},"outputs":[],"source":["\n","metrics_list = list_metrics()\n","print(metrics_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kv4m5UAf1B3"},"outputs":[],"source":["acc_metric = load_metric('accuracy')\n","f1_metric = load_metric('f1')\n","precision_metric = load_metric('precision')\n","recall_metric = load_metric('recall')"]},{"cell_type":"markdown","metadata":{},"source":["## Load Models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","model = \"microsoft/swinv2-base-patch4-window12-192-22k\"\n","\n","feature_extractor = AutoImageProcessor.from_pretrained(model)\n","feature_model = Swinv2Model.from_pretrained(model).to('cuda')"]},{"cell_type":"markdown","metadata":{"id":"7ZD53AeagF4D"},"source":["## Create Dataset function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4pIK2KYvSnh-"},"outputs":[],"source":["\n","class DatasetBuilder(Dataset):\n","    def __init__(self, df, tokenizer, sequence_length, \n","                 print_text=False):         \n","\n","        self.sequence_length = sequence_length\n","        self.tokenizer = tokenizer\n","        self.print_text = print_text\n","\n","        texts = df[\"text\"].values.tolist()\n","        labels = df[\"label\"].values.tolist()\n","        images = df[\"img\"].values.tolist()\n","        ids =  df[\"idx\"].values.tolist()\n","\n","        self.dataset = []\n","        for i, inp in enumerate(texts):\n","            self.dataset.append({\"text\": inp, \"label\": labels[i], 'idx': ids[i], 'image': images[i]})\n","  \n","    def __len__(self):\n","        return len(self.dataset)\n","\n","\n","    def tokenize_data(self, example):\n","   \n","        idx = example['idx']\n","        idx = [idx] if isinstance(idx, str) else idx\n","        encoded_dict = self.tokenizer(example['text'], padding='max_length', max_length=self.sequence_length, truncation=True, return_tensors='pt')\n","        tokens = encoded_dict['input_ids']\n","        token_type_ids = encoded_dict['token_type_ids']\n","        attn_mask = encoded_dict['attention_mask']\n","        \n","        targets = torch.tensor(example['label']).type(torch.int64)\n","\n","        ## Get Visual Embeddings\n","        try:\n","            img = example['image']\n","            img = Image.open(os.path.join('hateful_memes', img))\n","            img = np.array(img)\n","            img = img[...,:3]\n","            inputs = feature_extractor(images=img, return_tensors=\"pt\")\n","            outputs = feature_model(**inputs.to('cuda'))\n","            visual_embeds = outputs.last_hidden_state\n","            visual_embeds = visual_embeds.cpu()\n","        except:\n","            \n","            visual_embeds = np.zeros(shape=(36, 1024), dtype=float)\n","\n","\n","        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n","        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n","\n","        inputs={\"input_ids\": tokens.squeeze(),\n","            \"attention_mask\": attn_mask.squeeze(),\n","            \"token_type_ids\": token_type_ids.squeeze(),\n","            \"visual_embeds\": visual_embeds.squeeze(),\n","            \"visual_token_type_ids\": visual_token_type_ids.squeeze(),\n","            \"visual_attention_mask\": visual_attention_mask.squeeze(),\n","            \"label\": targets.squeeze()\n","        }\n","        \n","        return inputs\n","  \n","    def __getitem__(self, index):\n","        inputs = self.tokenize_data(self.dataset[index])\n","        \n","        if self.print_text:\n","            for k in inputs.keys():\n","                print(k, inputs[k].shape, inputs[k].dtype)\n","\n","        return inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W5W2mYO_kLLf"},"outputs":[],"source":["dataset = DatasetBuilder(df_val, tokenizer, 50, True)"]},{"cell_type":"markdown","metadata":{"id":"RAhhNY4y4MUa"},"source":["## Tuning using Pytorch Lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D8Nh2K8j4L1g"},"outputs":[],"source":["import pytorch_lightning as pl\n","from torch.utils.data import Dataset, DataLoader\n","from pytorch_lightning.loggers import WandbLogger\n","from datasets import load_metric\n","import torch.nn.functional as F\n","from sklearn.metrics import accuracy_score\n","from transformers import (\n","    AdamW,\n","    VisualBertModel,\n","    get_linear_schedule_with_warmup\n",")\n","import logging\n","import argparse\n","import time\n","from torch.nn import CrossEntropyLoss\n","from sklearn.metrics import roc_auc_score\n","\n","from transformers import BertTokenizer, VisualBertModel, TrainingArguments, Trainer, VisualBertConfig\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpsDvM-r4L4T"},"outputs":[],"source":["# from pytorch_lightning.loggers.wandb import WandbLogger\n","import os\n","from pathlib import Path\n","from string import punctuation\n","import torch.nn as nn"]},{"cell_type":"markdown","metadata":{"id":"J-hl24bz3obU"},"source":["## VisualBERT Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rTSuZE5G3FNu"},"outputs":[],"source":["from torch.autograd import grad\n","\n","class VisualBERT(torch.nn.Module):\n","    def __init__(self):\n","        \"\"\"\n","        Steps:\n","           1. Initialize pre-trained VisualBERT model.\n","           2. Initialize two nn.Linear layers to get desired tensor outputs.\n","           3. Initialize weights from weighted mean: [0.77510622, 1.40873991]\n","\n","        \"\"\"\n","        super(VisualBERT, self).__init__()\n","        configuration = VisualBertConfig.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre',\n","                                                hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1)\n","        self.visualbert = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre', config=configuration)\n","        self.embed_cls = nn.Linear(1024, 1024)\n","        # self.visualbert = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\n","        self.num_labels = 2\n","        self.dropout = nn.Dropout(0.3)\n","        self.cls=  nn.Linear(768, self.num_labels)\n","        self.weight = torch.FloatTensor([class_weights]) #torch.FloatTensor([0.77510622, 1.40873991]),\n","\n","        nSamples = [5178, 2849]\n","        normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n","        self.loss_fct = CrossEntropyLoss(weight=torch.FloatTensor(normedWeights))\n","        \n","    \n","    def forward(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask,\n","                visual_token_type_ids, labels):\n","        visual_embeds_cls = self.embed_cls(visual_embeds)\n","        #print('Input id Size: ',input_ids.shape)\n","        ##print('Attention Size Mask: ',attention_mask.shape)\n","        #print('Token Type id Size: ',token_type_ids.shape)\n","        #print('visual_embeds_cls Size: ',visual_embeds_cls.shape)\n","        #print('visual attention mask Size: ', visual_attention_mask.shape)\n","        #print('Visual Token Type id Size: ',visual_token_type_ids.shape) \n","\n","\n","\n","\n","        outputs = self.visualbert(\n","                input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids,\n","                visual_embeds=visual_embeds_cls,\n","                visual_attention_mask=visual_attention_mask,\n","                visual_token_type_ids=visual_token_type_ids,\n","            )\n","        \n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.cls(pooled_output)\n","        reshaped_logits = logits.view(-1, self.num_labels)\n","\n","        loss = self.loss_fct(reshaped_logits, labels.view(-1))\n","      \n","        #print('Current Loss: ',loss)\n","        #print('Current Loss Size: ',loss.shape)\n","        #print('Current Loss Size: ',len(loss))\n","        #print(pooled_output.shape)\n","        #print('Last Hidden State Size: ',outputs.last_hidden_state.shape)\n","        #print('Visual Embedding Size: ',visual_embeds_cls.shape)\n","        if pooled_output.requires_grad:\n","\n","            #print(outputs)\n","            perturbed_sentence = self.adv_attack(outputs.last_hidden_state, loss)\n","            #print('Perturbed Sentence: ',perturbed_sentence.shape)\n","            perturbed = self.adv_attack(visual_embeds_cls, loss)\n","            #print('Perturbed: ',perturbed.shape)\n","            mask =(attention_mask, visual_attention_mask)\n","            #print(mask.shape)\n","            perturbed = (perturbed_sentence, perturbed)\n","            #print('Attention Mask Size: ',attention_mask.shape)\n","            adv_loss = self.adversarial_loss(perturbed, input_ids,visual_embeds_cls, attention_mask, mask, labels, token_type_ids,visual_token_type_ids)\n","            loss = loss + adv_loss\n","            #print('Perturbed Loss Size: ',loss.shape)\n","        \n","\n","        return loss, reshaped_logits\n","    \n","    def adv_attack(self, emb, loss, epsilon=0.05):\n","        loss_grad = grad(loss, emb, retain_graph=True)[0]\n","        #print(\"Loss grad Size: \",loss_grad.shape)\n","        loss_grad_norm = torch.sqrt(torch.sum(loss_grad**2, (1,2)))\n","        perturbed_sentence = emb + epsilon * (loss_grad/(loss_grad_norm.reshape(-1,1,1)))\n","        \n","        return perturbed_sentence\n","\n","    def adversarial_loss(self, perturbed, input_ids, visual_embeds_cls, attention_mask, mask, labels, token_type_ids,visual_token_type_ids):\n","        #print(perturbed.shape)\n","        #print('\\n\\n===================================================================')\n","        #print('Before')\n","        #print('Adversarial Input Embed Size: ',input_ids.shape)\n","        #print('Adversarial Attention Mask Size: ',attention_mask.shape)\n","        #print('Adversarial Token Type Size: ',token_type_ids.shape)\n","        #print('Adversarial Visual Embed Size: ',perturbed[1].shape)\n","        #print('Adversarial Visual Attention Mask Size: ',mask[1].shape)\n","        #print('Adversarial Visual Token Type Size: ',visual_token_type_ids.shape)\n","        #print('Input id Size: ',perturbed[0][:, 0:512, :].shape)\n","        #print('Perturbed 1: ',perturbed[1].shape)\n","        #print('Perturbed Alt: ',perturbed[0][:, -36:, :].shape)\n","        out = self.visualbert(\n","                #input_ids = input_ids,\n","                inputs_embeds = perturbed[0][:, :50, :],\n","                attention_mask=attention_mask\n","                ,token_type_ids=token_type_ids\n","                ,visual_embeds = perturbed[1]\n","                ,visual_attention_mask=mask[1]\n","                ,visual_token_type_ids=visual_token_type_ids\n","            )\n","        \n","\n","        #print('After')\n","\n","\n","\n","        encoded_layers_last = out['pooler_output']\n","        encoded_layers_last = self.dropout(encoded_layers_last)\n","        logits = self.cls(encoded_layers_last)\n","        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-1)\n","        adv_loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","        return adv_loss\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2398,"status":"ok","timestamp":1646666255198,"user":{"displayName":"吴睿天","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyHoWC83255DLzqGIvhduYRpRsVBg62ekSrWVV=s64","userId":"16191779364250004927"},"user_tz":-480},"id":"Q6v7whCY5Qwy","outputId":"97084da7-51dd-4d3e-fc7c-c45cae9a68e6"},"outputs":[],"source":["model = VisualBERT().to('cuda')"]},{"cell_type":"markdown","metadata":{"id":"f7tblaBP7Gb_"},"source":["## HuggingFace Trainer "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q64H3ECW7Jq_"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","batch_size = 24\n","seq_len = 50\n","\n","\n","metric_name = \"auroc\"\n","\n","args = TrainingArguments(\n","    output_dir = \"OUTPUT_DIRECTORY\",\n","    seed = 4, \n","    evaluation_strategy = \"steps\",\n","    learning_rate=1e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs= 100,\n","    weight_decay=0.05,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=metric_name,\n","    eval_steps = 50,\n","    save_steps = 500,\n","    fp16 = False,\n","    gradient_accumulation_steps = 2\n",")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    acc = acc_metric.compute(predictions=predictions, references=labels)\n","    f1 = f1_metric.compute(predictions=predictions, references=labels)\n","    precision = precision_metric.compute(predictions=predictions, references=labels)\n","    recall = recall_metric.compute(predictions=predictions, references=labels)\n","    auc_score = roc_auc_score(labels, predictions)\n","    \n","    return {\"accuracy\": acc['accuracy'], \"auroc\": auc_score,'f1':f1['f1'],'precision':precision['precision'],'recall':recall['recall']} \n","\n","\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset = DatasetBuilder(df_train,tokenizer=tokenizer, sequence_length=seq_len),\n","    eval_dataset =  DatasetBuilder(df_val,tokenizer=tokenizer, sequence_length=seq_len),\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","resume_from = 'CHECKPOINT_LOCATION'\n","#trainer.train(resume_from)\n","trainer.train()\n","trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pm3hl5JY3k8"},"outputs":[],"source":["trainer.save_model('MODEL_SAVE_LOCATION')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"VisualBERT.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
