{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 8291, 'img': 'img/08291.png', 'label': 1, 'text': 'white people is this a shooting range'}\n",
      "{'id': 46971, 'img': 'img/46971.png', 'label': 1, 'text': 'bravery at its finest'}\n",
      "{'id': 3745, 'img': 'img/03745.png', 'label': 1, 'text': 'your order comes to $37.50 and your white privilege discount brings the total to $37.50'}\n",
      "{'id': 83745, 'img': 'img/83745.png', 'label': 1, 'text': 'it is time.. to send these parasites back to the desert'}\n",
      "{'id': 80243, 'img': 'img/80243.png', 'label': 1, 'text': 'mississippi wind chime'}\n",
      "{'id': 5279, 'img': 'img/05279.png', 'label': 1, 'text': \"knowing white people , that's probably the baby father\"}\n",
      "{'id': 1796, 'img': 'img/01796.png', 'label': 1, 'text': 'life hack #23 how to get stoned with no weed'}\n",
      "{'id': 53046, 'img': 'img/53046.png', 'label': 1, 'text': \"you've heard of elf on a shelf, now get ready for\"}\n",
      "{'id': 82301, 'img': 'img/82301.png', 'label': 1, 'text': 'cooooooooooooon!!!!'}\n",
      "{'id': 31752, 'img': 'img/31752.png', 'label': 1, 'text': 'when you get to choose your own mental illness'}\n",
      "{'id': 27635, 'img': 'img/27635.png', 'label': 1, 'text': 'dat ass ahmarbkrich*d'}\n",
      "{'id': 80597, 'img': 'img/80597.png', 'label': 1, 'text': 'look! it says it right here! we can fuck goats!'}\n",
      "{'id': 45368, 'img': 'img/45368.png', 'label': 1, 'text': 'enough is enough children are more important than freaks'}\n",
      "{'id': 17963, 'img': 'img/17963.png', 'label': 1, 'text': 'removes polish with chemicals nobody bats an eye removes polish with chemicals everybody loses his shit'}\n",
      "{'id': 53968, 'img': 'img/53968.png', 'label': 1, 'text': \"when you have an 80 point iq but it's the highest in your village\"}\n",
      "{'id': 10749, 'img': 'img/10749.png', 'label': 1, 'text': 'my sense of humor is so dark it picks cotton'}\n",
      "{'id': 25149, 'img': 'img/25149.png', 'label': 1, 'text': 'if this offends you you might be lack toes intolerant'}\n",
      "{'id': 87520, 'img': 'img/87520.png', 'label': 1, 'text': 'yes, we know trump said \"mexico will pay for the wall\" but now mexicans are the wall. that boy good!!!!'}\n",
      "{'id': 89071, 'img': 'img/89071.png', 'label': 1, 'text': \"they don't wash their ass with water and they eat their daughters' ass\"}\n",
      "{'id': 9563, 'img': 'img/09563.png', 'label': 1, 'text': 'safe sex isis style'}\n",
      "{'id': 72048, 'img': 'img/72048.png', 'label': 1, 'text': 'hurry up barack will be home soon'}\n",
      "{'id': 49826, 'img': 'img/49826.png', 'label': 1, 'text': 'white people on cinco de mayo vs. every other day'}\n",
      "{'id': 26453, 'img': 'img/26453.png', 'label': 1, 'text': \"a ww2 jewish prisoner trying to escape the auchschwitz showers (1943, colorized) wwii jew escaping hitler's rath\"}\n",
      "{'id': 12650, 'img': 'img/12650.png', 'label': 1, 'text': 'which one has tranny problems?'}\n",
      "{'id': 2568, 'img': 'img/02568.png', 'label': 1, 'text': 'whatsthe difference between heroin and i? cops think twice before shooting heroin'}\n",
      "{'id': 83954, 'img': 'img/83954.png', 'label': 1, 'text': 'head teachers given the power to ban the burqa in british schools share if you want a full ban in britain!'}\n",
      "{'id': 24316, 'img': 'img/24316.png', 'label': 1, 'text': \"sharks don't attack him why? they think it's whale shit\"}\n",
      "{'id': 62035, 'img': 'img/62035.png', 'label': 1, 'text': 'thank god my ass needed a break'}\n",
      "{'id': 65342, 'img': 'img/65342.png', 'label': 1, 'text': 'barn army enlist, and bomb ant hills \"grab your pitchfork and meet at the barn\"'}\n",
      "{'id': 92058, 'img': 'img/92058.png', 'label': 1, 'text': 'how black people became christians'}\n",
      "{'id': 58672, 'img': 'img/58672.png', 'label': 1, 'text': 'did you know that the chinese eat dogs savages disgusting'}\n",
      "{'id': 54069, 'img': 'img/54069.png', 'label': 1, 'text': 'when you decide to go full gas'}\n",
      "{'id': 91586, 'img': 'img/91586.png', 'label': 1, 'text': 'left the kitchen too many times'}\n",
      "{'id': 87130, 'img': 'img/87130.png', 'label': 1, 'text': 'finally finding the right tutorial you need indian accent'}\n",
      "{'id': 95640, 'img': 'img/95640.png', 'label': 1, 'text': 'syrian rape survivor speaks out'}\n",
      "{'id': 47819, 'img': 'img/47819.png', 'label': 1, 'text': \"oh, addiction isn't a disease because it started with a choice?? tell me more about how the chlamydia in your crotch isn't a disease, yet it started with a choice\"}\n",
      "{'id': 59140, 'img': 'img/59140.png', 'label': 1, 'text': 'wanna go masturbate? fuck yeah! masturbating!'}\n",
      "{'id': 13647, 'img': 'img/13647.png', 'label': 1, 'text': 'she said girls run the world... so i told her get back in the kitchen'}\n",
      "{'id': 65832, 'img': 'img/65832.png', 'label': 1, 'text': \"why love one but eat the other ? i don't get it\"}\n",
      "{'id': 57621, 'img': 'img/57621.png', 'label': 1, 'text': \"if this gets 1k likes i'll post to islam how we see muslims how muslims see us\"}\n",
      "{'id': 40982, 'img': 'img/40982.png', 'label': 1, 'text': 'and just like that... this sandwich maker doubles as ironing board'}\n",
      "{'id': 43275, 'img': 'img/43275.png', 'label': 1, 'text': 'my husband is always pushing me around... and talking behind my back'}\n",
      "{'id': 12973, 'img': 'img/12973.png', 'label': 1, 'text': 'someone added to much soap to the dishwasher again'}\n",
      "{'id': 74350, 'img': 'img/74350.png', 'label': 1, 'text': 'a head diaper is required when you have shit for brains'}\n",
      "{'id': 2478, 'img': 'img/02478.png', 'label': 1, 'text': 'must..resist...urge to.. kill whitey.'}\n",
      "{'id': 51607, 'img': 'img/51607.png', 'label': 1, 'text': '49 people were killed at christchurch muslim 49 people were killed at christchurch mosques muslim'}\n",
      "{'id': 65403, 'img': 'img/65403.png', 'label': 1, 'text': 'money is evil give it to us'}\n",
      "{'id': 29750, 'img': 'img/29750.png', 'label': 1, 'text': 'this cat is getting raped tonight'}\n",
      "{'id': 26547, 'img': 'img/26547.png', 'label': 1, 'text': \"that's the way allah allah i like it\"}\n",
      "{'id': 2143, 'img': 'img/02143.png', 'label': 1, 'text': 'doesnt have food, water, electricity proud of nuclear weapons'}\n",
      "{'id': 68192, 'img': 'img/68192.png', 'label': 1, 'text': 'scientist are working hard to cure them all'}\n",
      "{'id': 52603, 'img': 'img/52603.png', 'label': 1, 'text': 'feeling kinda cute might steal your shit later.. idk'}\n",
      "{'id': 5349, 'img': 'img/05349.png', 'label': 0, 'text': 'normal day in a islamic country'}\n",
      "{'id': 6491, 'img': 'img/06491.png', 'label': 1, 'text': 'tranny for sale'}\n",
      "{'id': 74013, 'img': 'img/74013.png', 'label': 1, 'text': \"nurse: yes sir, we are having mashed potatoes today alzheimer's patient: i don't remember asking you a god damn thing\"}\n",
      "{'id': 27485, 'img': 'img/27485.png', 'label': 1, 'text': 'thats the way allah allah i like it!!!'}\n",
      "{'id': 13026, 'img': 'img/13026.png', 'label': 1, 'text': 'under the name of multiculturalism we let terrorist into this country'}\n",
      "{'id': 17265, 'img': 'img/17265.png', 'label': 0, 'text': 'meanwhile in baltimore'}\n",
      "{'id': 91405, 'img': 'img/91405.png', 'label': 0, 'text': 'wears rag on her head so her brother husband doesnt want to molest her'}\n",
      "{'id': 14026, 'img': 'img/14026.png', 'label': 1, 'text': 'told girlfriend that mom is deaf so speak loud and slow told mom that girlfriend is retarded'}\n",
      "{'id': 18945, 'img': 'img/18945.png', 'label': 1, 'text': 'when you decide to go full gas'}\n",
      "{'id': 83920, 'img': 'img/83920.png', 'label': 1, 'text': 'just under 17 and a half years ago, 19 people were allowed into the us who should not have been all these years later, we are told that the answer to safety is to allow everyone in'}\n",
      "{'id': 83497, 'img': 'img/83497.png', 'label': 0, 'text': 'different types of radicalization'}\n",
      "{'id': 4569, 'img': 'img/04569.png', 'label': 0, 'text': 'when is the time to eat i am hungry'}\n",
      "{'id': 39607, 'img': 'img/39607.png', 'label': 1, 'text': 'those stupid basterds thought it was a real shower'}\n",
      "{'id': 9715, 'img': 'img/09715.png', 'label': 1, 'text': 'i started a business that sells landmines disguised as prayer mats.. ..prophets are going through the roof'}\n",
      "{'id': 1925, 'img': 'img/01925.png', 'label': 1, 'text': \"i am not racist i just don't like brown sugar\"}\n",
      "{'id': 94813, 'img': 'img/94813.png', 'label': 0, 'text': 'thats the way allah allah i like it!!!'}\n",
      "{'id': 29437, 'img': 'img/29437.png', 'label': 1, 'text': 'what can happen if you eat wheaties for 38 years'}\n",
      "{'id': 95038, 'img': 'img/95038.png', 'label': 0, 'text': 'when you take these pics we think about cumming in your mouth'}\n",
      "{'id': 64891, 'img': 'img/64891.png', 'label': 1, 'text': \"lebanon: before muslims after muslims but don't worry! after islam takes over there'll be a golden age.. apparently\"}\n",
      "{'id': 63280, 'img': 'img/63280.png', 'label': 1, 'text': \"so you're never going to forgive the germans because of the holocaust? same\"}\n",
      "{'id': 17682, 'img': 'img/17682.png', 'label': 1, 'text': 'give them what they deserve kill them all'}\n",
      "{'id': 84510, 'img': 'img/84510.png', 'label': 1, 'text': 'bartender: how about a nice 12 yr old? him: boy or girl?'}\n",
      "{'id': 28690, 'img': 'img/28690.png', 'label': 0, 'text': \"if you're gonna talk shit about someone... you better make sure they aren't crazier than you are\"}\n",
      "{'id': 3567, 'img': 'img/03567.png', 'label': 1, 'text': 'potato is my favorite color of the alphabet'}\n",
      "{'id': 95830, 'img': 'img/95830.png', 'label': 1, 'text': 'face of terrorism in us'}\n",
      "{'id': 29873, 'img': 'img/29873.png', 'label': 1, 'text': 'who knew that this country is full of white trash'}\n",
      "{'id': 70953, 'img': 'img/70953.png', 'label': 1, 'text': '\"god\\'s chosen people\" according to the book they wrote'}\n",
      "{'id': 56124, 'img': 'img/56124.png', 'label': 0, 'text': 'how to cook turkey in usa how to cook turkey in romania'}\n",
      "{'id': 10386, 'img': 'img/10386.png', 'label': 1, 'text': 'the book about terrorism'}\n",
      "{'id': 19530, 'img': 'img/19530.png', 'label': 1, 'text': 'they shot the wrong gorilla'}\n",
      "{'id': 94185, 'img': 'img/94185.png', 'label': 1, 'text': 'how a black woman can fight crime'}\n",
      "{'id': 48370, 'img': 'img/48370.png', 'label': 1, 'text': '72 virgins are ready to get fucked up'}\n",
      "{'id': 74058, 'img': 'img/74058.png', 'label': 0, 'text': 'he made millions of children homeless'}\n",
      "{'id': 45062, 'img': 'img/45062.png', 'label': 0, 'text': \"me and my kids are going to rob this country blind at the expense of the very people that voted for me. and they're going to love it. believe me! bigly!\"}\n",
      "{'id': 94170, 'img': 'img/94170.png', 'label': 0, 'text': 'when you decide to go full gas'}\n",
      "{'id': 4538, 'img': 'img/04538.png', 'label': 0, 'text': 'bows here, bows there, bows everywhere'}\n",
      "{'id': 83946, 'img': 'img/83946.png', 'label': 1, 'text': 'when a white kid shoots up the school the same day you planned to bomb it'}\n",
      "{'id': 68127, 'img': 'img/68127.png', 'label': 0, 'text': 'all you guys think about is dope'}\n",
      "{'id': 97453, 'img': 'img/97453.png', 'label': 0, 'text': 'potato is my favorite color of the alphabet'}\n",
      "{'id': 39578, 'img': 'img/39578.png', 'label': 1, 'text': 'one of the bullets they dug out of a victim from the orland gay night club shooting!'}\n",
      "{'id': 49360, 'img': 'img/49360.png', 'label': 0, 'text': 'who wants ice cream me me me'}\n",
      "{'id': 87034, 'img': 'img/87034.png', 'label': 0, 'text': \"you don't need a parachute to go skydiving twice you need a parachute to go skydiving twice\"}\n",
      "{'id': 6123, 'img': 'img/06123.png', 'label': 1, 'text': 'a group of men'}\n",
      "{'id': 17028, 'img': 'img/17028.png', 'label': 1, 'text': \"feeling bored, let's go to a mosque\"}\n",
      "{'id': 42903, 'img': 'img/42903.png', 'label': 0, 'text': 'those stupid basterds thought it was a real shower'}\n",
      "{'id': 71680, 'img': 'img/71680.png', 'label': 1, 'text': 'muslim strip club'}\n",
      "{'id': 24135, 'img': 'img/24135.png', 'label': 0, 'text': 'good night keep your butthole tight'}\n",
      "{'id': 32875, 'img': 'img/32875.png', 'label': 0, 'text': \"if he wears number one he's most likely the fastes on the team\"}\n",
      "{'id': 79085, 'img': 'img/79085.png', 'label': 1, 'text': 'today we are making vegetable soup'}\n",
      "{'id': 61872, 'img': 'img/61872.png', 'label': 1, 'text': \"don't be a fool it is a bomb\"}\n",
      "{'id': 30579, 'img': 'img/30579.png', 'label': 0, 'text': 'white power!'}\n",
      "{'id': 62504, 'img': 'img/62504.png', 'label': 0, 'text': \"we've taken this country! now fuck off! we're taking it back!\"}\n",
      "{'id': 43810, 'img': 'img/43810.png', 'label': 1, 'text': 'how to cook turkey in usa how to cook turkey in romania'}\n",
      "{'id': 19730, 'img': 'img/19730.png', 'label': 1, 'text': 'jamal is practicing for class sport'}\n",
      "{'id': 37420, 'img': 'img/37420.png', 'label': 0, 'text': 'like lebanon terrorism can destroy a beautiful country'}\n",
      "{'id': 82509, 'img': 'img/82509.png', 'label': 0, 'text': '72 virgins are ready to get fucked up'}\n",
      "{'id': 96284, 'img': 'img/96284.png', 'label': 1, 'text': 'welcome to melbourne'}\n",
      "{'id': 97068, 'img': 'img/97068.png', 'label': 1, 'text': \"these hitler jokes are rude anne frankly i don't care\"}\n",
      "{'id': 3214, 'img': 'img/03214.png', 'label': 0, 'text': 'bartender: how about a nice 12 yr old? him: boy or girl?'}\n",
      "{'id': 23810, 'img': 'img/23810.png', 'label': 1, 'text': \"these aren't people\"}\n",
      "{'id': 91836, 'img': 'img/91836.png', 'label': 1, 'text': \"one of my 5 boyfriends slept with another girl i'll never trust men again!\"}\n",
      "{'id': 5126, 'img': 'img/05126.png', 'label': 1, 'text': 'can you smell what the reich is cooking'}\n",
      "{'id': 76092, 'img': 'img/76092.png', 'label': 0, 'text': 'everybody is afraid of gangs'}\n",
      "{'id': 64072, 'img': 'img/64072.png', 'label': 1, 'text': 'trump said immigrants come here to steal your wife is a great example she stole my speech'}\n",
      "{'id': 91058, 'img': 'img/91058.png', 'label': 1, 'text': \"get in loser we're invading poland\"}\n",
      "{'id': 21643, 'img': 'img/21643.png', 'label': 0, 'text': 'haha pig man is peeing standing up'}\n",
      "{'id': 19523, 'img': 'img/19523.png', 'label': 0, 'text': 'they see them rollin.... they hating..'}\n",
      "{'id': 32691, 'img': 'img/32691.png', 'label': 1, 'text': 'this is how muslims celebrate new year'}\n",
      "{'id': 87169, 'img': 'img/87169.png', 'label': 1, 'text': 'albanians talking photo with a porn star'}\n",
      "{'id': 92738, 'img': 'img/92738.png', 'label': 1, 'text': 'they are the best in basketball because they can steal, shoot and run'}\n",
      "{'id': 53027, 'img': 'img/53027.png', 'label': 0, 'text': 'putting bows on your pet'}\n",
      "{'id': 27195, 'img': 'img/27195.png', 'label': 0, 'text': \"there's a reason we keep the beer in the bottom of the cooler\"}\n",
      "{'id': 43910, 'img': 'img/43910.png', 'label': 1, 'text': 'i just started a company selling landmines that look like prayer mats prophets are going through the roof'}\n",
      "{'id': 5213, 'img': 'img/05213.png', 'label': 0, 'text': 'the families of the hillsborough tragedy were denied legal aid.... but shamima begum gets it to fight for her uk citizenship our system is a sick joke! - martin costello'}\n",
      "{'id': 73945, 'img': 'img/73945.png', 'label': 0, 'text': 'jamal is holding a gun'}\n",
      "{'id': 56207, 'img': 'img/56207.png', 'label': 1, 'text': 'we broke up but she said we could still be cousins'}\n",
      "{'id': 68459, 'img': 'img/68459.png', 'label': 1, 'text': 'family trip in mexico'}\n",
      "{'id': 63175, 'img': 'img/63175.png', 'label': 0, 'text': \"if i was uncircumcised i'd hide candy in my dick skin so when a girl about to suck me up, she has a pleasant surprise.. oh my god, skittles\"}\n",
      "{'id': 82945, 'img': 'img/82945.png', 'label': 0, 'text': 'be careful who you call ugly in highschool'}\n",
      "{'id': 34975, 'img': 'img/34975.png', 'label': 0, 'text': 'what if the only reason mary got pregnant... is because god is gay and needed a surrogate'}\n",
      "{'id': 61973, 'img': 'img/61973.png', 'label': 1, 'text': 'what do alcoholics and necrophiliacs have in common they both love to crack open a cold one.'}\n",
      "{'id': 16420, 'img': 'img/16420.png', 'label': 0, 'text': 'abnormal vs normal'}\n",
      "{'id': 73069, 'img': 'img/73069.png', 'label': 1, 'text': 'immigration invasion learn the difference'}\n",
      "{'id': 47056, 'img': 'img/47056.png', 'label': 1, 'text': 'illegal aliens are not welcome'}\n",
      "{'id': 56428, 'img': 'img/56428.png', 'label': 0, 'text': \"if the shocker don't rock her spock her\"}\n",
      "{'id': 54206, 'img': 'img/54206.png', 'label': 0, 'text': 'you can smell disgust and hate from this photo'}\n",
      "{'id': 93172, 'img': 'img/93172.png', 'label': 0, 'text': 'this is a terrorist'}\n",
      "{'id': 56980, 'img': 'img/56980.png', 'label': 0, 'text': \"coffee isn't helping get the jumper cables\"}\n",
      "{'id': 79615, 'img': 'img/79615.png', 'label': 0, 'text': 'what women think men want what men really want'}\n",
      "{'id': 38047, 'img': 'img/38047.png', 'label': 0, 'text': '\"stupid bitch\" is not a race'}\n",
      "{'id': 94560, 'img': 'img/94560.png', 'label': 1, 'text': 'and for my next class project!'}\n",
      "{'id': 96250, 'img': 'img/96250.png', 'label': 0, 'text': \"i'm going home i'm russian\"}\n",
      "{'id': 28951, 'img': 'img/28951.png', 'label': 0, 'text': \"if the brim of your hat is flat and has a sticker on it i automatically assume you're a complete tool\"}\n",
      "{'id': 27614, 'img': 'img/27614.png', 'label': 0, 'text': \"maybe if you think before you act you'd be treated better\"}\n",
      "{'id': 4857, 'img': 'img/04857.png', 'label': 1, 'text': '...even the dumb jews know that! trump is a racist.'}\n",
      "{'id': 79603, 'img': 'img/79603.png', 'label': 1, 'text': 'its friday cunts lets get on the gas'}\n",
      "{'id': 26985, 'img': 'img/26985.png', 'label': 1, 'text': 'i just heard my girlfriend is pregnant'}\n",
      "{'id': 14873, 'img': 'img/14873.png', 'label': 1, 'text': 'human dna found in ancient goat carcass scientists believe this is first proof muhammad existed'}\n",
      "{'id': 54893, 'img': 'img/54893.png', 'label': 0, 'text': 'old people at weddings always poke me and say \"you\\'re next.\" so, i started doing the same thing to them at funerals'}\n",
      "{'id': 48236, 'img': 'img/48236.png', 'label': 0, 'text': \"that blood you donated could be in someone's boner right now\"}\n",
      "{'id': 74908, 'img': 'img/74908.png', 'label': 0, 'text': 'she is condemning shooting the gorilla'}\n",
      "{'id': 96472, 'img': 'img/96472.png', 'label': 1, 'text': 'white power!'}\n",
      "{'id': 89430, 'img': 'img/89430.png', 'label': 0, 'text': \"when you know you don't have to jackoff tonight\"}\n",
      "{'id': 76921, 'img': 'img/76921.png', 'label': 1, 'text': 'she probably asked a wrong question'}\n",
      "{'id': 68409, 'img': 'img/68409.png', 'label': 1, 'text': 'for the last time im not michelle obama'}\n",
      "{'id': 16354, 'img': 'img/16354.png', 'label': 1, 'text': \"we've taken this country! now fuck off! we're taking it back!\"}\n",
      "{'id': 69548, 'img': 'img/69548.png', 'label': 1, 'text': \"wait, mohammad come back i didn't mean to laugh, really size dosen't matter to me lol\"}\n",
      "{'id': 1456, 'img': 'img/01456.png', 'label': 1, 'text': 'they see them rollin..... they hating..'}\n",
      "{'id': 17950, 'img': 'img/17950.png', 'label': 0, 'text': 'i was excoriated for a harmless \"tweet\" and \"my show\" was cancelled'}\n",
      "{'id': 76295, 'img': 'img/76295.png', 'label': 0, 'text': 'islamic extremist planning new project'}\n",
      "{'id': 42580, 'img': 'img/42580.png', 'label': 1, 'text': 'jack and jill went up the hill so jack could lick her candy but jack got a shock and a mouthful of cock cause jills real name was randy'}\n",
      "{'id': 96180, 'img': 'img/96180.png', 'label': 1, 'text': 'putting bows on your pet'}\n",
      "{'id': 98701, 'img': 'img/98701.png', 'label': 1, 'text': \"oh no i can't steal anymore\"}\n",
      "{'id': 15243, 'img': 'img/15243.png', 'label': 0, 'text': '4 am. everyone is asleep perfect time to run from one end of the house to the other at full speed as loudly as possible'}\n",
      "{'id': 41058, 'img': 'img/41058.png', 'label': 0, 'text': 'john wilkes booth be like: yall sure abraham lincoln dead? he aint up here'}\n",
      "{'id': 62375, 'img': 'img/62375.png', 'label': 1, 'text': \"im not saying i'm racist im just saying all muslims are terrorists\"}\n",
      "{'id': 63507, 'img': 'img/63507.png', 'label': 0, 'text': 'evolution from peeling bananas to busting caps'}\n",
      "{'id': 31208, 'img': 'img/31208.png', 'label': 1, 'text': \"maybe if we weren't consistent sources of crime we'd be treated better\"}\n",
      "{'id': 62319, 'img': 'img/62319.png', 'label': 1, 'text': 'america in 2045 \"grandpa...why didn\\'t you stop them?!\"'}\n",
      "{'id': 68253, 'img': 'img/68253.png', 'label': 1, 'text': 'free blanket maybe white man not so bad after all'}\n",
      "{'id': 34687, 'img': 'img/34687.png', 'label': 1, 'text': 'everyone is afraid of dark'}\n",
      "{'id': 34209, 'img': 'img/34209.png', 'label': 0, 'text': 'how about a nice 12 yr old? bartender: him: boy i would love that'}\n",
      "{'id': 39827, 'img': 'img/39827.png', 'label': 1, 'text': 'kick them out of country'}\n",
      "{'id': 46920, 'img': 'img/46920.png', 'label': 1, 'text': 'we want sharia law !! well.... go home'}\n",
      "{'id': 84273, 'img': 'img/84273.png', 'label': 0, 'text': \"when you're the reason for the company safety video\"}\n",
      "{'id': 39018, 'img': 'img/39018.png', 'label': 0, 'text': 'she said \"i want a ring\" i said \"bitch take your phone off silent!\"'}\n",
      "{'id': 54780, 'img': 'img/54780.png', 'label': 1, 'text': 'travel size coffee maker'}\n",
      "{'id': 69150, 'img': 'img/69150.png', 'label': 1, 'text': '\"white privilege the privilege of being called \"racist\" by other people who see nothing else about you except the color of your skin'}\n",
      "{'id': 29843, 'img': 'img/29843.png', 'label': 0, 'text': \"the world's most wanted terrorist obama bin lying\"}\n",
      "{'id': 19385, 'img': 'img/19385.png', 'label': 0, 'text': \"rose's are red violet's are blue little stupid ass bitch i ain't fuckin witchu\"}\n",
      "{'id': 64510, 'img': 'img/64510.png', 'label': 0, 'text': 'mohammad won the contest in putting underwear on goats'}\n",
      "{'id': 47103, 'img': 'img/47103.png', 'label': 0, 'text': \"maybe we weren't consistent sources of crime we'd be treated better\"}\n",
      "{'id': 61038, 'img': 'img/61038.png', 'label': 0, 'text': 'no wonder why the pipes are not working'}\n",
      "{'id': 71620, 'img': 'img/71620.png', 'label': 1, 'text': 'we are returning our broken coffee maker'}\n",
      "{'id': 91602, 'img': 'img/91602.png', 'label': 0, 'text': 'upper downer'}\n",
      "{'id': 30145, 'img': 'img/30145.png', 'label': 0, 'text': 'before my presidency, there was no isis. there was no caliphate there was no blm. there was no antifa. there was no war on police i built that'}\n",
      "{'id': 91756, 'img': 'img/91756.png', 'label': 1, 'text': 'abnormal vs normal'}\n",
      "{'id': 63745, 'img': 'img/63745.png', 'label': 1, 'text': 'we could destroy all of them but we let some of them survive so you know why we did it'}\n",
      "{'id': 47162, 'img': 'img/47162.png', 'label': 1, 'text': 'what do these two have in common? they both are good at sucking they both come with a bag'}\n",
      "{'id': 59806, 'img': 'img/59806.png', 'label': 0, 'text': 'on tuesday, november 3, 2020, we make america great again  by voting this nightmare out of office!'}\n",
      "{'id': 7135, 'img': 'img/07135.png', 'label': 1, 'text': 'this is the greatest news manufacturing in 20th century'}\n",
      "{'id': 35497, 'img': 'img/35497.png', 'label': 0, 'text': 'old people at weddings always poke me and say \"you\\'re next.\" so, i started doing the same thing to them at funerals'}\n",
      "{'id': 84362, 'img': 'img/84362.png', 'label': 0, 'text': 'unleash the inner beast on the field'}\n",
      "{'id': 84015, 'img': 'img/84015.png', 'label': 1, 'text': 'my irony meter just exploded'}\n",
      "{'id': 87251, 'img': 'img/87251.png', 'label': 0, 'text': 'i love apes they are both ugly and cute'}\n",
      "{'id': 82590, 'img': 'img/82590.png', 'label': 1, 'text': 'burning them was a great public service'}\n",
      "{'id': 59738, 'img': 'img/59738.png', 'label': 1, 'text': \"what's the difference between a refugee and e.t e.t learned english and wanted to go home\"}\n",
      "{'id': 35470, 'img': 'img/35470.png', 'label': 0, 'text': 'can you smell what the reigh is cooking'}\n",
      "{'id': 72061, 'img': 'img/72061.png', 'label': 1, 'text': 'i like my rice crispy please'}\n",
      "{'id': 3524, 'img': 'img/03524.png', 'label': 1, 'text': 'terrorist can come to this country way to easy'}\n",
      "{'id': 86170, 'img': 'img/86170.png', 'label': 1, 'text': 'how to keep the thefts out'}\n",
      "{'id': 50841, 'img': 'img/50841.png', 'label': 0, 'text': 'give them what they deserve kill them all'}\n",
      "{'id': 46087, 'img': 'img/46087.png', 'label': 0, 'text': 'give that man a suicide'}\n",
      "{'id': 42058, 'img': 'img/42058.png', 'label': 0, 'text': 'for lunch vlad the impaler loved turkey'}\n",
      "{'id': 1967, 'img': 'img/01967.png', 'label': 0, 'text': 'when you decide to stop racism by burning its symbol'}\n",
      "{'id': 54930, 'img': 'img/54930.png', 'label': 1, 'text': 'not all muslims are isis! some are al-qaida the rest are taliban'}\n",
      "{'id': 38076, 'img': 'img/38076.png', 'label': 1, 'text': '\"he\\'s a squirter\" doesn\\'t sound as sexy when you are gay'}\n",
      "{'id': 38910, 'img': 'img/38910.png', 'label': 1, 'text': 'oh boy i sure love being muslim in the new zealand city of christchurch on march 15'}\n",
      "{'id': 6723, 'img': 'img/06723.png', 'label': 1, 'text': 'the hardest part to eat in a vegetable'}\n",
      "{'id': 2157, 'img': 'img/02157.png', 'label': 1, 'text': \"mohammed i'm pregnant.... don't sass me, you know it's funny! ~cupcakes\"}\n",
      "{'id': 62948, 'img': 'img/62948.png', 'label': 1, 'text': 'black lives matter >\"how\",\"why\"and \"to whom\" is unclear'}\n",
      "{'id': 21075, 'img': 'img/21075.png', 'label': 0, 'text': 'size of a shuttle bus legroom of a pinto'}\n",
      "{'id': 73601, 'img': 'img/73601.png', 'label': 1, 'text': 'when you are white and stole your first bike'}\n",
      "{'id': 32579, 'img': 'img/32579.png', 'label': 0, 'text': 'come to the dark side we have weed'}\n",
      "{'id': 21567, 'img': 'img/21567.png', 'label': 1, 'text': 'our dad is a potato farmer'}\n",
      "{'id': 1765, 'img': 'img/01765.png', 'label': 1, 'text': 'after a girl dies, what organ in her body stays warm? my dick'}\n",
      "{'id': 53172, 'img': 'img/53172.png', 'label': 0, 'text': 'straight outta the oven step one :turn on the gas'}\n",
      "{'id': 32568, 'img': 'img/32568.png', 'label': 1, 'text': 'america taught us how to be happy slaves the same way school teach us how to be happy employees'}\n",
      "{'id': 1742, 'img': 'img/01742.png', 'label': 0, 'text': \"in just one hour from now i'll only have 4 hours left until i have to work a mere 3 hours\"}\n",
      "{'id': 56098, 'img': 'img/56098.png', 'label': 1, 'text': 'i said \"a glass of juice\".'}\n",
      "{'id': 78612, 'img': 'img/78612.png', 'label': 0, 'text': 'go see fifty shades darker in theaters alone and throw mayonnaise at couples in the dark'}\n",
      "{'id': 34018, 'img': 'img/34018.png', 'label': 1, 'text': 'fuck the catholics fuck the catholic church fuck the pope'}\n",
      "{'id': 53609, 'img': 'img/53609.png', 'label': 0, 'text': 'when your friend insists on paying for dinner but you still want to contribute - just the tip, come on... let me'}\n",
      "{'id': 89362, 'img': 'img/89362.png', 'label': 1, 'text': 'has extra chromosome probably stole it'}\n",
      "{'id': 80426, 'img': 'img/80426.png', 'label': 1, 'text': 'a talking ape'}\n",
      "{'id': 83045, 'img': 'img/83045.png', 'label': 1, 'text': 'so much trash, we need a garbage collector'}\n",
      "{'id': 61503, 'img': 'img/61503.png', 'label': 0, 'text': 'someone else used kids to push an agenda'}\n",
      "{'id': 41890, 'img': 'img/41890.png', 'label': 1, 'text': 'house of pedophile'}\n",
      "{'id': 52091, 'img': 'img/52091.png', 'label': 1, 'text': 'women ask for equal rights, so i give them equal lefts aswell'}\n",
      "{'id': 24396, 'img': 'img/24396.png', 'label': 1, 'text': 'so like, why are men even gay? girls have buttholes too you know.'}\n",
      "{'id': 72168, 'img': 'img/72168.png', 'label': 1, 'text': 'upper downer'}\n",
      "{'id': 85761, 'img': 'img/85761.png', 'label': 1, 'text': 'no wonder why the pipes are not working'}\n",
      "{'id': 35719, 'img': 'img/35719.png', 'label': 0, 'text': 'jamal is practicing for class sport'}\n",
      "{'id': 90256, 'img': 'img/90256.png', 'label': 1, 'text': 'whenever you feel stupid, remember there are people who throw stones at a wall! aiming for an invisible devil, told by an invisible god for an invisible heaven!'}\n",
      "{'id': 20984, 'img': 'img/20984.png', 'label': 0, 'text': 'when you are white and stole your first bike'}\n",
      "{'id': 95086, 'img': 'img/95086.png', 'label': 1, 'text': 'novaky concentration camp these jews can claim holocaust pensions'}\n",
      "{'id': 49028, 'img': 'img/49028.png', 'label': 0, 'text': 'i finally found my girlfriends g-spot turns out it was in her sister the whole time'}\n",
      "{'id': 18356, 'img': 'img/18356.png', 'label': 0, 'text': 'i took a shower this morning'}\n",
      "{'id': 52634, 'img': 'img/52634.png', 'label': 0, 'text': 'the secret to a successful marriage is to keep his stomach full and his balls empty'}\n",
      "{'id': 80512, 'img': 'img/80512.png', 'label': 1, 'text': \"you can make jokes about anything, just not illegals that's crossing the border\"}\n",
      "{'id': 62703, 'img': 'img/62703.png', 'label': 0, 'text': 'they shot the wrong gorilla'}\n",
      "{'id': 95176, 'img': 'img/95176.png', 'label': 0, 'text': 'notice how the kenyan skidmark has been silent about the mueller report?'}\n",
      "{'id': 27384, 'img': 'img/27384.png', 'label': 0, 'text': \"that face you make when you're a bully and just got beat up by a girl\"}\n",
      "{'id': 82437, 'img': 'img/82437.png', 'label': 1, 'text': 'we should learn from ocean how to treat refugees'}\n",
      "{'id': 26943, 'img': 'img/26943.png', 'label': 1, 'text': 'my incesters built this country!!!!'}\n",
      "{'id': 9152, 'img': 'img/09152.png', 'label': 0, 'text': 'i don\\'t have ex\\'s. i have y\\'s. like, \"y the hell did i date you?!?\"'}\n",
      "{'id': 7429, 'img': 'img/07429.png', 'label': 1, 'text': \"i'm about to have my 5th abortion fuck you batman i'm the real crime fighter\"}\n",
      "{'id': 75639, 'img': 'img/75639.png', 'label': 1, 'text': 'changing every single country it touches'}\n",
      "{'id': 46812, 'img': 'img/46812.png', 'label': 0, 'text': 'goat meat is very popular in islamic countries'}\n",
      "{'id': 63827, 'img': 'img/63827.png', 'label': 1, 'text': 'teacher of the year for black community'}\n",
      "{'id': 41276, 'img': 'img/41276.png', 'label': 1, 'text': 'evolution from peeling bananas to busting caps'}\n",
      "{'id': 56413, 'img': 'img/56413.png', 'label': 1, 'text': 'feeling cute might shoot a darkie later idk'}\n",
      "{'id': 69512, 'img': 'img/69512.png', 'label': 0, 'text': 'happy birthday you sexy beast'}\n",
      "{'id': 43175, 'img': 'img/43175.png', 'label': 1, 'text': 'when was the last time you saw so many mentally ill people on the streat'}\n",
      "{'id': 54129, 'img': 'img/54129.png', 'label': 0, 'text': 'dump pipe to the ocean'}\n",
      "{'id': 23645, 'img': 'img/23645.png', 'label': 1, 'text': 'what do they have in common? you enjoy pushing them down the stairs'}\n",
      "{'id': 89425, 'img': 'img/89425.png', 'label': 1, 'text': \"hello chocolate paradise princess i'm albert\"}\n",
      "{'id': 16923, 'img': 'img/16923.png', 'label': 1, 'text': 'do you like goat anus? smell my finger'}\n",
      "{'id': 4769, 'img': 'img/04769.png', 'label': 1, 'text': \"if she's pouting like this and you don't immediately drop the argument , motherfucker you gay\"}\n",
      "{'id': 69815, 'img': 'img/69815.png', 'label': 1, 'text': 'thank you for allowing me into your country now we need to talk about the things that need to change around here to accommodate my beliefs'}\n",
      "{'id': 91468, 'img': 'img/91468.png', 'label': 0, 'text': 'you want a whore? buy one. you want a queen? earn her. you want a nickel bag? fifteen bucks, little man'}\n",
      "{'id': 52104, 'img': 'img/52104.png', 'label': 0, 'text': \"lebanon: before islam after islam but don't worry! after islam takes over, there'll be a golden age... apparently\"}\n",
      "{'id': 52031, 'img': 'img/52031.png', 'label': 0, 'text': \"theriously?? that's methed up!\"}\n",
      "{'id': 8795, 'img': 'img/08795.png', 'label': 0, 'text': 'when you are white and working in oil field'}\n",
      "{'id': 50317, 'img': 'img/50317.png', 'label': 1, 'text': 'what in the hell is wrong with america voting towel heads into congress'}\n",
      "{'id': 73914, 'img': 'img/73914.png', 'label': 1, 'text': 'be careful who you call ugly in highschool'}\n",
      "{'id': 3197, 'img': 'img/03197.png', 'label': 1, 'text': \"i'm no bird expert but i'm guessing it's 4 females and 1 male\"}\n",
      "{'id': 49621, 'img': 'img/49621.png', 'label': 0, 'text': \"wait, mohammad come back i didn't mean to laugh, really size dosen't matter to me lol\"}\n",
      "{'id': 68530, 'img': 'img/68530.png', 'label': 1, 'text': 'a fast sandwich maker'}\n",
      "{'id': 98547, 'img': 'img/98547.png', 'label': 0, 'text': \"waves mexican flag doesn't want to live in mexico\"}\n",
      "{'id': 3519, 'img': 'img/03519.png', 'label': 1, 'text': \"i'm going home i'm russian\"}\n",
      "{'id': 43698, 'img': 'img/43698.png', 'label': 0, 'text': \"don't be late for work get the protestor plow\"}\n",
      "{'id': 84756, 'img': 'img/84756.png', 'label': 1, 'text': 'when you ask the kid that misses class a lot why he is bald and he starts crying all right. then keep vour secrets'}\n",
      "{'id': 93051, 'img': 'img/93051.png', 'label': 0, 'text': 'and for my next class project!'}\n",
      "{'id': 68257, 'img': 'img/68257.png', 'label': 0, 'text': 'everyone is afraid of dark'}\n",
      "{'id': 12834, 'img': 'img/12834.png', 'label': 1, 'text': 'in west philadelphia, born and raised in the anus is where i got most of my aids'}\n",
      "{'id': 57208, 'img': 'img/57208.png', 'label': 0, 'text': 'i had potato for lunch'}\n",
      "{'id': 26439, 'img': 'img/26439.png', 'label': 0, 'text': \"parental opioid use has negative effect on children's intelligence\"}\n",
      "{'id': 53769, 'img': 'img/53769.png', 'label': 0, 'text': 'in 1979 jimmy carter created the department of education since then the u.s. has gone from 1st to 17th in education'}\n",
      "{'id': 28406, 'img': 'img/28406.png', 'label': 0, 'text': 'when she gives you that look'}\n",
      "{'id': 53418, 'img': 'img/53418.png', 'label': 0, 'text': 'a real man loads the dishwasher every night'}\n",
      "{'id': 10785, 'img': 'img/10785.png', 'label': 1, 'text': 'time to put the \"panic\" back in hispanic'}\n",
      "{'id': 84302, 'img': 'img/84302.png', 'label': 0, 'text': 'why do we share more when these people are hurt but not these people'}\n",
      "{'id': 76015, 'img': 'img/76015.png', 'label': 0, 'text': 'everyone celebrates christmas !'}\n",
      "{'id': 34189, 'img': 'img/34189.png', 'label': 0, 'text': 'fight for what you believe in fight for the future'}\n",
      "{'id': 52079, 'img': 'img/52079.png', 'label': 0, 'text': \"just bought a house! now we're house-poor\"}\n",
      "{'id': 63987, 'img': 'img/63987.png', 'label': 0, 'text': 'the migrant caravan arrives!'}\n",
      "{'id': 73526, 'img': 'img/73526.png', 'label': 0, 'text': \"if she's still making noises, you didn't hit her hard enough\"}\n",
      "{'id': 2145, 'img': 'img/02145.png', 'label': 0, 'text': \"and then i asked mom, what's for dinner?\"}\n",
      "{'id': 84102, 'img': 'img/84102.png', 'label': 0, 'text': 'the definition of utter disgust in plain black and white'}\n",
      "{'id': 32981, 'img': 'img/32981.png', 'label': 0, 'text': 'the latest and greatest. a truck that comes with a dishwasher!'}\n",
      "{'id': 73605, 'img': 'img/73605.png', 'label': 0, 'text': 'no matter what smile'}\n",
      "{'id': 7382, 'img': 'img/07382.png', 'label': 1, 'text': \"wouldn't be the last time those people used the oven this way\"}\n",
      "{'id': 64071, 'img': 'img/64071.png', 'label': 0, 'text': \"when you have someone to help and take care of you what could happen if you didn't\"}\n",
      "{'id': 80912, 'img': 'img/80912.png', 'label': 0, 'text': 'mr obama it is an honor'}\n",
      "{'id': 84762, 'img': 'img/84762.png', 'label': 1, 'text': 'do you know how to turn your dishwasher into a snowblower? throw em a shovel'}\n",
      "{'id': 39076, 'img': 'img/39076.png', 'label': 0, 'text': 'playing with the monkeys in thailand'}\n",
      "{'id': 85679, 'img': 'img/85679.png', 'label': 1, 'text': 'mr obama it is an honor'}\n",
      "{'id': 38095, 'img': 'img/38095.png', 'label': 0, 'text': \"haters will say it's photoshop\"}\n",
      "{'id': 52394, 'img': 'img/52394.png', 'label': 0, 'text': 'i forgot what are we fighting over again?'}\n",
      "{'id': 14865, 'img': 'img/14865.png', 'label': 0, 'text': 'terrible as hitler was, he did enjoy watching sports'}\n",
      "{'id': 54819, 'img': 'img/54819.png', 'label': 0, 'text': 'beauty shot of girlfriends focusing on different thoughts and ideas'}\n",
      "{'id': 46082, 'img': 'img/46082.png', 'label': 0, 'text': 'the future of the democratic party'}\n",
      "{'id': 73962, 'img': 'img/73962.png', 'label': 1, 'text': 'you should treat that the way you treat you vacuum cleaner when it stops sucking change the bag'}\n",
      "{'id': 78462, 'img': 'img/78462.png', 'label': 0, 'text': 'in the last days there will be scoffers, ridiculers, deniers and mockers following after their own lusts'}\n",
      "{'id': 49805, 'img': 'img/49805.png', 'label': 0, 'text': 'imagine being so disgusting there have to be laws to try to stop normal people from hating you'}\n",
      "{'id': 24098, 'img': 'img/24098.png', 'label': 1, 'text': 'thanksgiving in china'}\n",
      "{'id': 67435, 'img': 'img/67435.png', 'label': 1, 'text': 'when you ask a jewish girl for her number and she starts rolling her sleeve'}\n",
      "{'id': 60893, 'img': 'img/60893.png', 'label': 0, 'text': 'busy, like a hive of bees, controlling what your family sees and edits all the news your hear so you\\'ll know what to think and fear till all the truth there is, is gone. that is unless you stand, en masse. \"'}\n",
      "{'id': 93148, 'img': 'img/93148.png', 'label': 0, 'text': 'the proper way to pop a blackhead'}\n",
      "{'id': 7351, 'img': 'img/07351.png', 'label': 0, 'text': '\"plays uno steals all the green cards\"'}\n",
      "{'id': 16850, 'img': 'img/16850.png', 'label': 0, 'text': \"okay okay okay here's my serious face do i look fuhrerious yet?\"}\n",
      "{'id': 96312, 'img': 'img/96312.png', 'label': 0, 'text': 'when you support your little one every step of the way'}\n",
      "{'id': 86195, 'img': 'img/86195.png', 'label': 0, 'text': 'dolled up and rolling with the gang like'}\n",
      "{'id': 84162, 'img': 'img/84162.png', 'label': 0, 'text': 'i could have killed all of them. but i let some of them survive so you know why i killed them'}\n",
      "{'id': 12067, 'img': 'img/12067.png', 'label': 0, 'text': 'pre-heat to 350 degrees'}\n",
      "{'id': 5938, 'img': 'img/05938.png', 'label': 1, 'text': 'in the muslim world all gay men are well hung'}\n",
      "{'id': 43092, 'img': 'img/43092.png', 'label': 0, 'text': 'pretty much sums up islam'}\n",
      "{'id': 30148, 'img': 'img/30148.png', 'label': 1, 'text': \"you mean i don't have to pick it?!?\"}\n",
      "{'id': 12045, 'img': 'img/12045.png', 'label': 0, 'text': \"learn to hunt. it's a valuable skill\"}\n",
      "{'id': 92068, 'img': 'img/92068.png', 'label': 0, 'text': 'still better than mexican'}\n",
      "{'id': 73021, 'img': 'img/73021.png', 'label': 0, 'text': 'when mom asks where all the jews went'}\n",
      "{'id': 56241, 'img': 'img/56241.png', 'label': 1, 'text': '94% of them test positive for rh blood type which means they have monkey blood as a result they are born with a tail and other animal traits'}\n",
      "{'id': 28905, 'img': 'img/28905.png', 'label': 1, 'text': \"hey, i just met you and this is crazy but here's your number.. so camping, maybe?\"}\n",
      "{'id': 13750, 'img': 'img/13750.png', 'label': 0, 'text': \"wouldn't be the last time those people used the oven this way\"}\n",
      "{'id': 6582, 'img': 'img/06582.png', 'label': 1, 'text': \"they are terrorists anyone who doesn't agree is an idiot\"}\n",
      "{'id': 75142, 'img': 'img/75142.png', 'label': 0, 'text': 'you should treat that the way you treat your vacuum cleaner, when it stops sucking change the bag'}\n",
      "{'id': 56149, 'img': 'img/56149.png', 'label': 0, 'text': 'the original scarecrow'}\n",
      "{'id': 41796, 'img': 'img/41796.png', 'label': 0, 'text': \"nein danke i'm very happy with my gas supplier\"}\n",
      "{'id': 31570, 'img': 'img/31570.png', 'label': 1, 'text': 'busy, like a hive of bees, controlling what your family sees and edits all the news your hear so you\\'ll know what to think and fear till all the truth there is, is gone that is unless you stand, en masse. \"'}\n",
      "{'id': 1726, 'img': 'img/01726.png', 'label': 1, 'text': 'a real man loads the dishwasher every night'}\n",
      "{'id': 23785, 'img': 'img/23785.png', 'label': 0, 'text': 'hansel and gretel was a scary childrens story'}\n",
      "{'id': 56473, 'img': 'img/56473.png', 'label': 0, 'text': 'knows about you. your family your children everything'}\n",
      "{'id': 62970, 'img': 'img/62970.png', 'label': 0, 'text': 'if you want to know how they were treated illustration of man with four slaves chained together'}\n",
      "{'id': 41035, 'img': 'img/41035.png', 'label': 0, 'text': 'my new muslim girlfriend not much to look at'}\n",
      "{'id': 10285, 'img': 'img/10285.png', 'label': 1, 'text': 'you are what you eat meanwhile in africa'}\n",
      "{'id': 95604, 'img': 'img/95604.png', 'label': 1, 'text': \"they want to be accepted for who they are, yet theren't able to accept themselves for who they are\"}\n",
      "{'id': 68401, 'img': 'img/68401.png', 'label': 0, 'text': 'when you date an asian boy and you trynna get his family to accept you'}\n",
      "{'id': 50984, 'img': 'img/50984.png', 'label': 0, 'text': 'a cat in a pot'}\n",
      "{'id': 17908, 'img': 'img/17908.png', 'label': 0, 'text': 'making a quick 3pm snack'}\n",
      "{'id': 42631, 'img': 'img/42631.png', 'label': 1, 'text': \"i believe they are like racism just shouldn't exist\"}\n",
      "{'id': 61349, 'img': 'img/61349.png', 'label': 1, 'text': 'the proper way to pop a blackhead'}\n",
      "{'id': 65093, 'img': 'img/65093.png', 'label': 0, 'text': 'cookie kunty seen in paris'}\n",
      "{'id': 86357, 'img': 'img/86357.png', 'label': 0, 'text': 'apparently not erdogan'}\n",
      "{'id': 74965, 'img': 'img/74965.png', 'label': 0, 'text': 'some people some other people'}\n",
      "{'id': 75918, 'img': 'img/75918.png', 'label': 0, 'text': \"a picture of my great great great grand grandfather's tractors they don't make them like they used to\"}\n",
      "{'id': 14975, 'img': 'img/14975.png', 'label': 0, 'text': \"hitting is a crime. it's about time people understood that\"}\n",
      "{'id': 92317, 'img': 'img/92317.png', 'label': 0, 'text': 'you either die a hero, or live long enough to become the villain'}\n",
      "{'id': 19536, 'img': 'img/19536.png', 'label': 1, 'text': 'when your jewish friend smells a stash of coins in public'}\n",
      "{'id': 90236, 'img': 'img/90236.png', 'label': 0, 'text': 'voila! fresh batch of baked goods !'}\n",
      "{'id': 30642, 'img': 'img/30642.png', 'label': 0, 'text': 'rabbi sitting in synagogue chillin and doing his thing'}\n",
      "{'id': 59206, 'img': 'img/59206.png', 'label': 0, 'text': 'traveling through chimney'}\n",
      "{'id': 19075, 'img': 'img/19075.png', 'label': 0, 'text': 'when you smell cake'}\n",
      "{'id': 79042, 'img': 'img/79042.png', 'label': 0, 'text': 'the door to the crematorium and gas chambers are closed'}\n",
      "{'id': 98720, 'img': 'img/98720.png', 'label': 0, 'text': \"when your wife just died giving birth to your daughter and you're alone in the room with your naked just born daughter\"}\n",
      "{'id': 2634, 'img': 'img/02634.png', 'label': 1, 'text': 'and just like that... this sandwich maker doubles as an ironing board'}\n",
      "{'id': 84107, 'img': 'img/84107.png', 'label': 0, 'text': 'i ordered a dishwasher and they sent me a sandwich maker'}\n",
      "{'id': 7198, 'img': 'img/07198.png', 'label': 1, 'text': 'islam is a religion of peace stop criticizing my religion'}\n",
      "{'id': 45702, 'img': 'img/45702.png', 'label': 1, 'text': \"if she's still making noises, you didn't hit her hard enough\"}\n",
      "{'id': 46085, 'img': 'img/46085.png', 'label': 0, 'text': 'city workers repair statue cover it up with black sheet'}\n",
      "{'id': 8451, 'img': 'img/08451.png', 'label': 0, 'text': 'when you ask a jewish girl for her number and she starts rolling her sleeve'}\n",
      "{'id': 73482, 'img': 'img/73482.png', 'label': 0, 'text': \"it's time we took guns away and put them out of the reach of our children\"}\n",
      "{'id': 28017, 'img': 'img/28017.png', 'label': 1, 'text': 'say it! say it ! islam is the religion of peace'}\n",
      "{'id': 2364, 'img': 'img/02364.png', 'label': 0, 'text': 'a jewish shop owner, wears a kippah or yarmulke whilst in his shop in tehran'}\n",
      "{'id': 42538, 'img': 'img/42538.png', 'label': 0, 'text': 'and at that moment everything else in the world just melted away'}\n",
      "{'id': 47183, 'img': 'img/47183.png', 'label': 0, 'text': 'taking the liberals to a whole new level'}\n",
      "{'id': 72904, 'img': 'img/72904.png', 'label': 1, 'text': 'terrorist trojan horse enemy of the state'}\n",
      "{'id': 90243, 'img': 'img/90243.png', 'label': 1, 'text': 'best gamer ever 6 million kills, 1 death'}\n",
      "{'id': 41382, 'img': 'img/41382.png', 'label': 0, 'text': \"they want to be accepted for who they are, yet theren't able to accept themselves for who they are\"}\n",
      "{'id': 78156, 'img': 'img/78156.png', 'label': 0, 'text': \"you mean i don't have to pick it?!?\"}\n",
      "{'id': 74250, 'img': 'img/74250.png', 'label': 0, 'text': 'white kids kids dreaming about their first day of school'}\n",
      "{'id': 97132, 'img': 'img/97132.png', 'label': 0, 'text': 'in the muslim world all gay men are well hung'}\n",
      "{'id': 35840, 'img': 'img/35840.png', 'label': 1, 'text': 'i ordered a dishwasher and they sent me a sandwich maker'}\n",
      "{'id': 78134, 'img': 'img/78134.png', 'label': 0, 'text': 'in other countries in pakistan'}\n",
      "{'id': 93620, 'img': 'img/93620.png', 'label': 1, 'text': 'in other countries in pakistan'}\n",
      "{'id': 57369, 'img': 'img/57369.png', 'label': 0, 'text': 'lose 15 lbs with this 1 simple trick photoshop'}\n",
      "{'id': 10362, 'img': 'img/10362.png', 'label': 1, 'text': \"german british danish austrian diversity. apparently we don't have it and these guys do: iraqi pakistani syrian egyptian\"}\n",
      "{'id': 36058, 'img': 'img/36058.png', 'label': 0, 'text': 'love is love'}\n",
      "{'id': 9467, 'img': 'img/09467.png', 'label': 1, 'text': 'when mom asks where all the jews went'}\n",
      "{'id': 60183, 'img': 'img/60183.png', 'label': 1, 'text': \"nein danke i'm very happy with my gas supplier\"}\n",
      "{'id': 47263, 'img': 'img/47263.png', 'label': 1, 'text': 'pre-heat to 350 degrees'}\n",
      "{'id': 18726, 'img': 'img/18726.png', 'label': 0, 'text': 'when your bed is short and your feed stick out'}\n",
      "{'id': 50379, 'img': 'img/50379.png', 'label': 0, 'text': 'do you know how to turn your dishwasher into a snowblower? throw em a shovel'}\n",
      "{'id': 6352, 'img': 'img/06352.png', 'label': 0, 'text': 'and then i asked mom, can we go play?'}\n",
      "{'id': 58093, 'img': 'img/58093.png', 'label': 0, 'text': 'i treat them like java treats its generic parameters at run time as objects'}\n",
      "{'id': 19508, 'img': 'img/19508.png', 'label': 1, 'text': 'the definition of utter disgust in plain black and white'}\n",
      "{'id': 2783, 'img': 'img/02783.png', 'label': 1, 'text': \"a picture of my great great great grand grandfather's tractors they don't make them like they used to\"}\n",
      "{'id': 41728, 'img': 'img/41728.png', 'label': 0, 'text': \"i believe they are like racism. just shouldn't exist\"}\n",
      "{'id': 51306, 'img': 'img/51306.png', 'label': 0, 'text': \"eat some cookies you're not yourself when you're hungry\"}\n",
      "{'id': 50241, 'img': 'img/50241.png', 'label': 1, 'text': 'bitches be like no weave, no makeup, no lashes, no eyebrows'}\n",
      "{'id': 49650, 'img': 'img/49650.png', 'label': 1, 'text': \"and then i asked mom, what's for dinner?\"}\n",
      "{'id': 17045, 'img': 'img/17045.png', 'label': 0, 'text': 'celebrating the presidential election'}\n",
      "{'id': 89536, 'img': 'img/89536.png', 'label': 0, 'text': 'so many lives lost because of this man'}\n",
      "{'id': 26187, 'img': 'img/26187.png', 'label': 1, 'text': 'i love my slow cooker'}\n",
      "{'id': 74906, 'img': 'img/74906.png', 'label': 1, 'text': \"are you a boy or a girl i'm non binary\"}\n",
      "{'id': 28190, 'img': 'img/28190.png', 'label': 1, 'text': 'when finishing a race is your passion'}\n",
      "{'id': 89642, 'img': 'img/89642.png', 'label': 1, 'text': 'i treat them like java treats its generic parameters at run time as objects'}\n",
      "{'id': 59170, 'img': 'img/59170.png', 'label': 0, 'text': 'and just like that... this sandwich maker doubles as an ironing board'}\n",
      "{'id': 30927, 'img': 'img/30927.png', 'label': 0, 'text': 'doesnt let anything stop her have a great day'}\n",
      "{'id': 32049, 'img': 'img/32049.png', 'label': 0, 'text': \"german british danish austrian diversity. apparently we don't have it and these guys do: iraqi, pakistani, syrian, egyptian\"}\n",
      "{'id': 95173, 'img': 'img/95173.png', 'label': 0, 'text': 'take the third reich and then the first left?'}\n",
      "{'id': 50198, 'img': 'img/50198.png', 'label': 0, 'text': \"you don't need cocaine to be happy\"}\n",
      "{'id': 37658, 'img': 'img/37658.png', 'label': 1, 'text': 'when your dishwasher is broken so you take it back to walmart to get a new one'}\n",
      "{'id': 95613, 'img': 'img/95613.png', 'label': 1, 'text': 'what a weird way to spell fat and single'}\n",
      "{'id': 43805, 'img': 'img/43805.png', 'label': 1, 'text': 'the original scarecrow'}\n",
      "{'id': 18306, 'img': 'img/18306.png', 'label': 1, 'text': 'white kids kids dreaming about their first day of school'}\n",
      "{'id': 46518, 'img': 'img/46518.png', 'label': 1, 'text': \"it's the goat humper, looks like a bunch of pigs just walked over your grave\"}\n",
      "{'id': 18547, 'img': 'img/18547.png', 'label': 1, 'text': '1st day of 4th grade sandy hook elementary school'}\n",
      "{'id': 2518, 'img': 'img/02518.png', 'label': 0, 'text': 'when the dog bites you and you bite back to assert dominance'}\n",
      "{'id': 4621, 'img': 'img/04621.png', 'label': 0, 'text': 'you are what you eat meanwhile in africa'}\n",
      "{'id': 98543, 'img': 'img/98543.png', 'label': 1, 'text': 'upper downer'}\n",
      "{'id': 85621, 'img': 'img/85621.png', 'label': 0, 'text': \"no, that's not his daughter... that's his wife! yet the world is silent..\"}\n",
      "{'id': 18367, 'img': 'img/18367.png', 'label': 0, 'text': 'i love my slow cooker'}\n",
      "{'id': 84036, 'img': 'img/84036.png', 'label': 0, 'text': 'militant dressed as martyr'}\n",
      "{'id': 86417, 'img': 'img/86417.png', 'label': 1, 'text': 'i could have killed all of them. but i let some of them survive so you know why i killed them'}\n",
      "{'id': 92567, 'img': 'img/92567.png', 'label': 0, 'text': 'pray for peace pray for wellbeing'}\n",
      "{'id': 63921, 'img': 'img/63921.png', 'label': 0, 'text': 'taking a photo with family'}\n",
      "{'id': 72864, 'img': 'img/72864.png', 'label': 1, 'text': 'traveling through chimney'}\n",
      "{'id': 30586, 'img': 'img/30586.png', 'label': 0, 'text': \"just can't get rid of the stain. this is why i hate beets\"}\n",
      "{'id': 83264, 'img': 'img/83264.png', 'label': 1, 'text': 'randomly deciding to bake something'}\n",
      "{'id': 93041, 'img': 'img/93041.png', 'label': 0, 'text': '\"who\\'s gonna make the sandwiches? what do we do with all these sandwiches?\"'}\n",
      "{'id': 97305, 'img': 'img/97305.png', 'label': 1, 'text': \"when your wife just died giving birth to your daughter and you're alone in the room with your naked just born daughter\"}\n",
      "{'id': 3568, 'img': 'img/03568.png', 'label': 0, 'text': 'time to play rich vs the constitution'}\n",
      "{'id': 7653, 'img': 'img/07653.png', 'label': 1, 'text': 'in the last days there will be god hating scoffers, ridiculers, deniers and mockers following after their own lusts'}\n",
      "{'id': 26397, 'img': 'img/26397.png', 'label': 0, 'text': 'there are still parts of the world where people celebrate capital punishment'}\n",
      "{'id': 3217, 'img': 'img/03217.png', 'label': 0, 'text': 'they are like sperm you get millions, but only one works'}\n",
      "{'id': 40618, 'img': 'img/40618.png', 'label': 0, 'text': 'when your company valuation hits 1 billion'}\n",
      "{'id': 67208, 'img': 'img/67208.png', 'label': 0, 'text': 'does nike also sell stones in case they take it off?'}\n",
      "{'id': 3798, 'img': 'img/03798.png', 'label': 0, 'text': \"hey, i just met you and this is crazy but here's your number... so camping, maybe?\"}\n",
      "{'id': 12468, 'img': 'img/12468.png', 'label': 0, 'text': 'say it! say it ! islam is the religion of peace'}\n",
      "{'id': 16842, 'img': 'img/16842.png', 'label': 0, 'text': 'every kitchen needs a sandwich maker just like this one'}\n",
      "{'id': 68043, 'img': 'img/68043.png', 'label': 1, 'text': 'plays uno steals all the green cards'}\n",
      "{'id': 51807, 'img': 'img/51807.png', 'label': 0, 'text': \"are you a boy or a girl i'm non binary\"}\n",
      "{'id': 78914, 'img': 'img/78914.png', 'label': 0, 'text': 'terrorist trojan horse enemy of the state'}\n",
      "{'id': 47950, 'img': 'img/47950.png', 'label': 0, 'text': \"democrats ilhan omar alexandria ocasio-cortez ayanna pressley rashida tlais we're new, bold, and we hate you! we hate whites, jews, christians, conservatives, republicans,\"}\n",
      "{'id': 95487, 'img': 'img/95487.png', 'label': 0, 'text': 'watching vegetables grow'}\n",
      "{'id': 49260, 'img': 'img/49260.png', 'label': 0, 'text': \"it's the goat humper looks like a bunch of pigs just walked over your grave\"}\n",
      "{'id': 20861, 'img': 'img/20861.png', 'label': 1, 'text': 'pretty much sums up islam'}\n",
      "{'id': 38914, 'img': 'img/38914.png', 'label': 0, 'text': 'cotton candy is what makes the theme park wonderful'}\n",
      "{'id': 47016, 'img': 'img/47016.png', 'label': 1, 'text': 'last summer camp was so \"lit\"'}\n",
      "{'id': 14389, 'img': 'img/14389.png', 'label': 1, 'text': \"democrats we're new, bold, and we hate you! alexandria ocasio-cortez rashida tlais ilhan omar ayanna pressley we hate whites, jews, christians, conservatives, republicans\"}\n",
      "{'id': 35487, 'img': 'img/35487.png', 'label': 0, 'text': 'randomly deciding to bake something'}\n",
      "{'id': 84150, 'img': 'img/84150.png', 'label': 0, 'text': 'when your jewish friend smells a stash of coins in public'}\n",
      "{'id': 13809, 'img': 'img/13809.png', 'label': 1, 'text': \"doing o's with the smoke\"}\n",
      "{'id': 50261, 'img': 'img/50261.png', 'label': 1, 'text': \"haters will say it's photoshop\"}\n",
      "{'id': 75286, 'img': 'img/75286.png', 'label': 1, 'text': 'look at this sandwich maker club i found on wish on clearance'}\n",
      "{'id': 64312, 'img': 'img/64312.png', 'label': 1, 'text': \"no, that's not his daughter.. that's his wife! yet the world is silent..\"}\n",
      "{'id': 67103, 'img': 'img/67103.png', 'label': 1, 'text': 'still better than mexican'}\n",
      "{'id': 18059, 'img': 'img/18059.png', 'label': 1, 'text': 'they are like sperm you get millions, but only one works'}\n",
      "{'id': 90643, 'img': 'img/90643.png', 'label': 0, 'text': 'she represents the new wave of democrats'}\n",
      "{'id': 16749, 'img': 'img/16749.png', 'label': 0, 'text': 'broken dishwasher'}\n",
      "{'id': 37814, 'img': 'img/37814.png', 'label': 0, 'text': \"this is el paso shooter's father every. single. time\"}\n",
      "{'id': 92046, 'img': 'img/92046.png', 'label': 0, 'text': \"they are terrorists anyone who doesn't agree is an idiot\"}\n",
      "{'id': 90843, 'img': 'img/90843.png', 'label': 0, 'text': 'time to put the \"panic\" back in hispanic'}\n",
      "{'id': 85237, 'img': 'img/85237.png', 'label': 0, 'text': 'these are all the weapons that were seized from isis'}\n",
      "{'id': 98235, 'img': 'img/98235.png', 'label': 0, 'text': 'fashion forward !'}\n",
      "{'id': 90267, 'img': 'img/90267.png', 'label': 0, 'text': \"i've been driving this baby for years the mileage isn't great but still gets the job done\"}\n",
      "{'id': 48296, 'img': 'img/48296.png', 'label': 1, 'text': 'when you date an asian boy and you trynna get his family to accept you'}\n",
      "{'id': 29174, 'img': 'img/29174.png', 'label': 0, 'text': 'the crimes against humanity should not be swiftly forgotten'}\n",
      "{'id': 65801, 'img': 'img/65801.png', 'label': 0, 'text': 'first friday prayers of the islamic holy day'}\n",
      "{'id': 91763, 'img': 'img/91763.png', 'label': 1, 'text': 'imagine being so disgusting there have to be laws to try to stop normal people from hating you'}\n",
      "{'id': 94387, 'img': 'img/94387.png', 'label': 0, 'text': \"i'm a fan of the color black on you although the rose gold looks pretty good as well\"}\n",
      "{'id': 80947, 'img': 'img/80947.png', 'label': 1, 'text': 'my new muslim girlfriend not much to look at'}\n",
      "{'id': 78251, 'img': 'img/78251.png', 'label': 1, 'text': 'watching vegetables grow'}\n",
      "{'id': 94738, 'img': 'img/94738.png', 'label': 0, 'text': 'shopping is boring. why not have some fun at the walmart parking lot?'}\n",
      "{'id': 57823, 'img': 'img/57823.png', 'label': 0, 'text': 'bitches be like no weave, no makeup, no lashes, no eyebrows & no filter. all natural'}\n",
      "{'id': 32415, 'img': 'img/32415.png', 'label': 0, 'text': 'when finishing a race is your passion'}\n",
      "{'id': 5316, 'img': 'img/05316.png', 'label': 0, 'text': \"doing o's with the smoke\"}\n",
      "{'id': 61085, 'img': 'img/61085.png', 'label': 0, 'text': 'upper downer'}\n",
      "{'id': 27498, 'img': 'img/27498.png', 'label': 1, 'text': \"who's gonna make the sandwiches? what do we do with all these sandwiches?\"}\n",
      "{'id': 37160, 'img': 'img/37160.png', 'label': 0, 'text': 'best gamer ever 6 million kills, 1 death\"'}\n",
      "{'id': 18742, 'img': 'img/18742.png', 'label': 0, 'text': \"i'm proud of it! come at me!\"}\n",
      "{'id': 19243, 'img': 'img/19243.png', 'label': 0, 'text': 'surprise!! wait, where is everyone?'}\n",
      "{'id': 54108, 'img': 'img/54108.png', 'label': 1, 'text': 'does nike also sell stones in case they take it off?'}\n",
      "{'id': 93528, 'img': 'img/93528.png', 'label': 1, 'text': 'portable dishwasher slash sandwich maker'}\n",
      "{'id': 78659, 'img': 'img/78659.png', 'label': 0, 'text': 'what a weird way to spell fat and single'}\n",
      "{'id': 67082, 'img': 'img/67082.png', 'label': 0, 'text': '\"1st day of 4th grade sandy hook elementary school\"'}\n",
      "{'id': 64125, 'img': 'img/64125.png', 'label': 0, 'text': \"ooof. that's gotta hurt\"}\n",
      "{'id': 41296, 'img': 'img/41296.png', 'label': 0, 'text': '94% of them test positive for rh blood type which means they have monkey blood as a result they are born with a tail and other animal traits'}\n",
      "{'id': 53491, 'img': 'img/53491.png', 'label': 0, 'text': 'and in 3 days he came back to life bringing salvation'}\n",
      "{'id': 37692, 'img': 'img/37692.png', 'label': 0, 'text': '\"islam is a religion of peace stop criticizing my religion\"'}\n",
      "{'id': 36201, 'img': 'img/36201.png', 'label': 0, 'text': 'look at me bling up my prosthetic'}\n",
      "{'id': 29054, 'img': 'img/29054.png', 'label': 1, 'text': 'broken dishwasher'}\n",
      "{'id': 6273, 'img': 'img/06273.png', 'label': 1, 'text': 'the jihad squad'}\n",
      "{'id': 25061, 'img': 'img/25061.png', 'label': 1, 'text': \"this is el paso shooter's father. every. single. time\"}\n",
      "{'id': 20437, 'img': 'img/20437.png', 'label': 0, 'text': 'thanksgiving in china'}\n",
      "{'id': 5439, 'img': 'img/05439.png', 'label': 0, 'text': 'portable dishwasher slash sandwich maker'}\n",
      "{'id': 34528, 'img': 'img/34528.png', 'label': 1, 'text': 'every kitchen needs a sandwich maker just like this one'}\n",
      "{'id': 93541, 'img': 'img/93541.png', 'label': 0, 'text': 'the jihad squad'}\n",
      "{'id': 16704, 'img': 'img/16704.png', 'label': 0, 'text': 'last summer camp was so \"lit\"'}\n",
      "{'id': 48792, 'img': 'img/48792.png', 'label': 0, 'text': 'leaving a trail of death and destruction'}\n",
      "{'id': 7528, 'img': 'img/07528.png', 'label': 1, 'text': 'the latest and greatest. a truck that comes with a dishwasher!'}\n",
      "{'id': 1268, 'img': 'img/01268.png', 'label': 0, 'text': 'father and daughter bonding time is the best like and share if you agree'}\n",
      "{'id': 83675, 'img': 'img/83675.png', 'label': 0, 'text': \"i'm gonna be like phelps one day\"}\n",
      "{'id': 37198, 'img': 'img/37198.png', 'label': 0, 'text': \"when you're so relaxed you can feel yourself go to the next level\"}\n",
      "{'id': 48670, 'img': 'img/48670.png', 'label': 0, 'text': 'look at this sandwich maker club i found on wish on clearance'}\n",
      "{'id': 9863, 'img': 'img/09863.png', 'label': 0, 'text': 'diverse group of women'}\n",
      "{'id': 97320, 'img': 'img/97320.png', 'label': 0, 'text': '\"when your dishwasher is broken so you take it back to walmart to get a new one\"'}\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import os\n",
    "\n",
    "parent_directory = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "data_directory = os.path.join(parent_directory,'Hateful-Memes/data')\n",
    "\n",
    "# Open the JSONL file in read mode using jsonlines\n",
    "with jsonlines.open(os.path.join(data_directory,'dev.jsonl')) as reader:\n",
    "    # Iterate over each line in the file\n",
    "    for data in reader:\n",
    "       \n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Transformer model from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VisualBert and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import BertTokenizer, VisualBertModel\n",
    "\n",
    "model = VisualBertModel.from_pretrained('uclanlp/visualbert-vqa-coco-pre')\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Swinv2Model for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Swinv2Model,AutoImageProcessor\n",
    "\n",
    "viz_model = Swinv2Model.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")#.from_pretrained('yusx-swapp/ofm-swinv2-base-patch4-window7-cifar100')\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")#.from_pretrained('yusx-swapp/ofm-swinv2-base-patch4-window7-cifar100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForImageClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m processor \u001b[39m=\u001b[39m AutoImageProcessor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/swinv2-base-patch4-window12-192-22k\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m AutoModelForImageClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/swinv2-base-patch4-window12-192-22k\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForImageClassification' is not defined"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for huggingface/cats-image contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/huggingface/cats-image\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 64, 768]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "model = Swinv2Model.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create DataFrame from train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import jsonlines\n",
    "\n",
    "parent_directory = os.path.dirname(os.getcwd())\n",
    "data_directory = os.path.join(parent_directory,'data')\n",
    "\n",
    "train_dataset = pd.DataFrame(columns=['id','img','label','text'])\n",
    "\n",
    "# Open the JSONL file in read mode using jsonlines\n",
    "with jsonlines.open(os.path.join(data_directory,'train.jsonl')) as reader:\n",
    "    # Iterate over each line in the file\n",
    "    for data in reader:\n",
    "        # Process the data as needed\n",
    "        data_df = pd.DataFrame([data])\n",
    "        train_dataset = pd.concat([train_dataset,data_df]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swinv2 vs. ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 1024])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os \n",
    "\n",
    "image = Image.open(os.path.join(data_directory,train_dataset.loc[0,'img']))\n",
    "image_array = np.array(image)\n",
    "tensor = torch.tensor(image_array)\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")\n",
    "viz_model = Swinv2Model.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")\n",
    "\n",
    "inputs = image_processor(tensor, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = viz_model(**inputs)\n",
    "\n",
    "visual_embeds = outputs.last_hidden_state\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "print(visual_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "from PIL import Image\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "feature_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "inputs = feature_extractor(images=tensor, return_tensors=\"pt\")\n",
    "outputs = feature_model(**inputs)#.to('cuda'))\n",
    "\n",
    "visual_embeds = outputs['last_hidden_state']\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "print(visual_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create input dataset for VisualBERT by connecting all the pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = train_dataset.loc[0,'text']\n",
    "input_text_tokenized = tokenizer(input_text, return_tensors='pt', padding='max_length', max_length=512, truncation=True)\n",
    "input_text_tokenized.update(\n",
    "    {\n",
    "        \"visual_embeds\": visual_embeds,\n",
    "        \"visual_token_type_ids\": visual_token_type_ids,\n",
    "        \"visual_attention_mask\": visual_attention_mask\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (197x768 and 2048x768)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m VisualBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muclanlp/visualbert-vqa-coco-pre\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#model = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_text_tokenized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\pgmw9\\Documents\\Georgia Tech\\DL\\Project\\Hateful-Memes\\projectenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pgmw9\\Documents\\Georgia Tech\\DL\\Project\\Hateful-Memes\\projectenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pgmw9\\Documents\\Georgia Tech\\DL\\Project\\Hateful-Memes\\projectenv\\Lib\\site-packages\\transformers\\models\\visual_bert\\modeling_visual_bert.py:805\u001b[0m, in \u001b[0;36mVisualBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, visual_embeds, visual_attention_mask, visual_token_type_ids, image_text_alignment, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    803\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 805\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisual_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisual_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisual_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisual_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_text_alignment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_text_alignment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbypass_transformer \u001b[38;5;129;01mand\u001b[39;00m visual_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    816\u001b[0m     text_length \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pgmw9\\Documents\\Georgia Tech\\DL\\Project\\Hateful-Memes\\projectenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pgmw9\\Documents\\Georgia Tech\\DL\\Project\\Hateful-Memes\\projectenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pgmw9\\Documents\\Georgia Tech\\DL\\Project\\Hateful-Memes\\projectenv\\Lib\\site-packages\\transformers\\models\\visual_bert\\modeling_visual_bert.py:140\u001b[0m, in \u001b[0;36mVisualBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, visual_embeds, visual_token_type_ids, image_text_alignment)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visual_token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     visual_token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\n\u001b[0;32m    137\u001b[0m         visual_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    138\u001b[0m     )\n\u001b[1;32m--> 140\u001b[0m visual_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m visual_token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_token_type_embeddings(visual_token_type_ids)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_text_alignment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# image_text_alignment = Batch x image_length x alignment_number.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# Each element denotes the position of the word corresponding to the image feature. -1 is the padding value.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pgmw9\\Documents\\Georgia Tech\\DL\\Project\\Hateful-Memes\\projectenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pgmw9\\Documents\\Georgia Tech\\DL\\Project\\Hateful-Memes\\projectenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pgmw9\\Documents\\Georgia Tech\\DL\\Project\\Hateful-Memes\\projectenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (197x768 and 2048x768)"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import BertTokenizer, VisualBertModel\n",
    "\n",
    "model = VisualBertModel.from_pretrained('uclanlp/visualbert-vqa-coco-pre')\n",
    "#model = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\n",
    "output = model(**input_text_tokenized)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of VisualBERT input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      1. Text input \n",
    "      2. Image input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Text input requires three pieces: input_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - input_ids: represent the token IDs of the input tokens after tokenization. Each token in the input text is converted into a numerical ID based on the tokenizer's vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - attention_mask: indicates which tokens should be attended to during processing. Binary mask where each position corresponds to a token in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - token_type_ids: represent the segment IDs for token types in the context of sequence pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Image input requires three pieces: visual_embeds, visual_token_type_ids, visual_attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - visual_embeds: represents the embeddings of visual features in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - visual_token_type_ids: represent the token type IDs for visual toekens. In multi-modal transformers distinguishes between tokens representing visual features and tokens representing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - vision_attention_mask: guide the attention mechanism for visual tokens. Specifies which tokens should be attended to during processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Probably dont need what's below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "from transformers import BertTokenizer, VisualBertModel\n",
    "from transformers import Swinv2Model,AutoImageProcessor\n",
    "import os \n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DatasetBuilder():\n",
    "    def __init__(self,model = None,tokenizer = None,json_file = None):\n",
    "        self.parent_directory = os.path.dirname(os.getcwd())\n",
    "        self.data_directory = os.path.join(self.parent_directory,'data')\n",
    "        self.json_file = json_file\n",
    "        print(tokenizer)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer)\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(model)\n",
    "        self.viz_model = Swinv2Model.from_pretrained(model)\n",
    "        self.dataset = self.create_dataframe()\n",
    "\n",
    "    def create_dataframe(self):\n",
    "        dataset = pd.DataFrame(columns=['id','img','label','text'])\n",
    "        with jsonlines.open(os.path.join(self.data_directory,self.json_file)) as reader:\n",
    "            for data in reader:\n",
    "                data_df = pd.DataFrame([data])\n",
    "                dataset = pd.concat([dataset,data_df]).reset_index(drop=True)\n",
    "\n",
    "        dataset['img'] = (self.data_directory+'/'+dataset['img']).apply(self.load_image)\n",
    "        return dataset\n",
    "    \n",
    "\n",
    "    def tokenize_data(self,value):\n",
    "        input = self.tokenizer(value['text'], return_tensors='pt', padding='max_length', max_length=512, truncation=True)\n",
    "        target = torch.tensor(value['label']).type(torch.int64)\n",
    "        \n",
    "        \n",
    "        image = self.image_processor(value['img'], return_tensors=\"pt\")\n",
    "            \n",
    "            \n",
    "        outputs = self.viz_model(**image)\n",
    "\n",
    "        visual_embeds = outputs.last_hidden_state\n",
    "\n",
    "       # except:\n",
    "       #     visual_embeds = np.zeros(shape=(197, 768), dtype=float)\n",
    "\n",
    "            \n",
    "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "        input.update(\n",
    "            {\n",
    "                \"visual_embeds\": visual_embeds,\n",
    "                \"visual_token_type_ids\": visual_token_type_ids,\n",
    "                \"visual_attention_mask\": visual_attention_mask,\n",
    "                \"label\":target\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return input\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        return self.dataset\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset.loc[index])\n",
    "        \n",
    "        for k in inputs.keys():\n",
    "            print(k, inputs[k].shape, inputs[k].dtype)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def load_image(self,filepath):\n",
    "        image = Image.open(filepath)\n",
    "        image_array = np.array(image)\n",
    "        return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "batch_size = 24\n",
    "seq_len = 50\n",
    "\n",
    "metric_name = \"auroc\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"model-checkpoint\",\n",
    "    seed = 110, \n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs= 100,\n",
    "    weight_decay=0.05,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    eval_steps = 50,\n",
    "    save_steps = 500,\n",
    "    fp16 = False,\n",
    "    gradient_accumulation_steps = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23032/3211214775.py:3: FutureWarning: list_metrics is deprecated and will be removed in the next major version of datasets. Use 'evaluate.list_evaluation_modules' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metrics_list = list_metrics()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy', 'bertscore', 'bleu', 'bleurt', 'brier_score', 'cer', 'character', 'charcut_mt', 'chrf', 'code_eval', 'comet', 'competition_math', 'confusion_matrix', 'coval', 'cuad', 'exact_match', 'f1', 'frugalscore', 'glue', 'google_bleu', 'indic_glue', 'mae', 'mahalanobis', 'mape', 'mase', 'matthews_correlation', 'mauve', 'mean_iou', 'meteor', 'mse', 'nist_mt', 'pearsonr', 'perplexity', 'poseval', 'precision', 'r_squared', 'recall', 'rl_reliability', 'roc_auc', 'rouge', 'sacrebleu', 'sari', 'seqeval', 'smape', 'spearmanr', 'squad', 'squad_v2', 'super_glue', 'ter', 'trec_eval', 'wer', 'wiki_split', 'xnli', 'xtreme_s', 'Aledade/extraction_evaluation', 'AlhitawiMohammed22/CER_Hu-Evaluation-Metrics', 'Bekhouche/NED', 'BucketHeadP65/confusion_matrix', 'BucketHeadP65/roc_curve', 'CZLC/rouge_raw', 'DaliaCaRo/accents_unplugged_eval', 'DarrenChensformer/action_generation', 'DarrenChensformer/eval_keyphrase', 'DarrenChensformer/relation_extraction', 'DoctorSlimm/bangalore_score', 'DoctorSlimm/kaushiks_criteria', 'Drunper/metrica_tesi', 'Felipehonorato/eer', 'Fritz02/execution_accuracy', 'GMFTBY/dailydialog_evaluate', 'GMFTBY/dailydialogevaluate', 'He-Xingwei/sari_metric', 'Ikala-allen/relation_extraction', 'JP-SystemsX/nDCG', 'Josh98/nl2bash_m', 'KevinSpaghetti/accuracyk', 'LottieW/accents_unplugged_eval', 'LuckiestOne/valid_efficiency_score', 'Merle456/accents_unplugged_eval', 'Muennighoff/code_eval_octopack', 'NCSOFT/harim_plus', 'Natooz/ece', 'Ndyyyy/bertscore', 'NikitaMartynov/spell-check-metric', 'NimaBoscarino/weat', 'Ochiroo/rouge_mn', 'Pipatpong/perplexity', 'Qui-nn/accents_unplugged_eval', 'RiciHuggingFace/accents_unplugged_eval', 'SEA-AI/det-metrics', 'SEA-AI/mot-metrics', 'Soroor/cer', 'SpfIo/wer_checker', 'Splend1dchan/cosine_similarity', 'TelEl/accents_unplugged_eval', 'Vallp/ter', 'Vertaix/vendiscore', 'Vickyage/accents_unplugged_eval', 'Viona/fuzzy_reordering', 'Viona/infolm', 'Viona/kendall_tau', 'Vipitis/shadermatch', 'Vlasta/pr_auc', 'Yeshwant123/mcc', 'abdusah/aradiawer', 'abidlabs/mean_iou', 'abidlabs/mean_iou2', 'agkphysics/ccc', 'akki2825/accents_unplugged_eval', 'alvinasvk/accents_unplugged_eval', 'amitness/perplexity', 'andstor/code_perplexity', 'angelasophie/accents_unplugged_eval', 'angelina-wang/directional_bias_amplification', 'anz2/iliauniiccocrevaluation', 'arthurvqin/pr_auc', 'aryopg/roc_auc_skip_uniform_labels', 'bascobasculino/mot-metrics', 'bdsaglam/jer', 'berkatil/map', 'boschar/accents_unplugged_eval', 'brian920128/doc_retrieve_metrics', 'bstrai/classification_report', 'buelfhood/fbeta_score', 'bugbounty1806/accuracy', 'cakiki/ndcg', 'carletoncognitivescience/peak_signal_to_noise_ratio', 'chanelcolgate/average_precision', 'chimene/accents_unplugged_eval', 'ckb/unigram', 'codeparrot/apps_metric', 'cpllab/syntaxgym', 'd-matrix/dmx_perplexity', 'daiyizheng/valid', 'danasone/ru_errant', 'danieldux/hierarchical_softmax_loss', 'danieldux/isco_hierarchical_accuracy', 'davebulaval/meaningbert', 'dayil100/accents_unplugged_eval', 'dayil100/accents_unplugged_eval_WER', 'dgfh76564/accents_unplugged_eval', 'dvitel/codebleu', 'ecody726/bertscore', 'erntkn/dice_coefficient', 'fnvls/bleu1234', 'fnvls/bleu_1234', 'franzi2505/detection_metric', 'fschlatt/ner_eval', 'gabeorlanski/bc_eval', 'ginic/phone_errors', 'giulio98/code_eval_outputs', 'giulio98/codebleu', 'gjacob/bertimbauscore', 'gjacob/chrf', 'gjacob/google_bleu', 'gjacob/wiki_split', 'gnail/cosine_similarity', 'gorkaartola/metric_for_tp_fp_samples', 'guydav/restrictedpython_code_eval', 'hack/test_metric', 'haotongye-shopee/ppl', 'harshhpareek/bertscore', 'helena-balabin/youden_index', 'hpi-dhc/FairEval', 'huanghuayu/multiclass_brier_score', 'hynky/sklearn_proxy', 'hyperml/balanced_accuracy', 'iNeil77/code_eval_octopack', 'idsedykh/codebleu', 'idsedykh/codebleu2', 'idsedykh/megaglue', 'idsedykh/metric', 'illorca/FairEval', 'ingyu/klue_mrc', 'jialinsong/apps_metric', 'jjkim0807/code_eval', 'jordyvl/ece', 'jpxkqx/peak_signal_to_noise_ratio', 'jpxkqx/signal_to_reconstruction_error', 'juliakaczor/accents_unplugged_eval', 'jzm-mailchimp/joshs_second_test_metric', 'k4black/codebleu', 'kashif/mape', 'kedudzic/charmatch', 'kyokote/my_metric2', 'langdonholmes/cohen_weighted_kappa', 'leslyarun/fbeta_score', 'lhy/hamming_loss', 'lhy/ranking_loss', 'livvie/accents_unplugged_eval', 'loubnabnl/apps_metric2', 'lvwerra/accuracy_score', 'lvwerra/bary_score', 'lvwerra/test', 'maksymdolgikh/seqeval_with_fbeta', 'manueldeprada/beer', 'mfumanelli/geometric_mean', 'mgfrantz/roc_auc_macro', 'mtc/fragments', 'nevikw39/specificity', 'nlpln/tst', 'ola13/precision_at_k', 'omidf/squad_precision_recall', 'posicube/mean_reciprocal_rank', 'red1bluelost/evaluate_genericify_cpp', 'repllabs/mean_average_precision', 'repllabs/mean_reciprocal_rank', 'ronaldahmed/nwentfaithfulness', 'saicharan2804/my_metric', 'sakusakumura/bertscore', 'shalakasatheesh/squad', 'shalakasatheesh/squad_v2', 'shirayukikun/sescore', 'shunzh/apps_metric', 'sma2023/wil', 'sportlosos/sescore', 'transZ/sbert_cosine', 'transZ/test_parascore', 'transformersegmentation/segmentation_scores', 'unitxt/metric', 'unnati/kendall_tau_distance', 'vichyt/metric-codebleu', 'vineelpratap/cer', 'weiqis/pajm', 'xu1998hz/sescore', 'xu1998hz/sescore_english_coco', 'xu1998hz/sescore_english_mt', 'xu1998hz/sescore_english_webnlg', 'xu1998hz/sescore_german_mt', 'ybelkada/cocoevaluate', 'yonting/average_precision_score', 'yqsong/execution_accuracy', 'yulong-me/yl_metric', 'yuyijiong/quad_match_score', 'yzha/ctc_eval', 'zbeloki/m2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23032/3211214775.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n",
      "  acc_metric = load_metric('accuracy')\n",
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datasets import list_metrics, load_metric\n",
    "metrics_list = list_metrics()\n",
    "print(metrics_list)\n",
    "\n",
    "\n",
    "acc_metric = load_metric('accuracy')\n",
    "f1_metric = load_metric('f1')\n",
    "precision_metric = load_metric('precision')\n",
    "recall_metric = load_metric('recall')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)\n",
    "    auc_score = roc_auc_score(labels, predictions)\n",
    "    return {\"accuracy\": acc['accuracy'], \"auroc\": auc_score,'f1':f1['f1'],'precision':precision['precision'],'recall':recall['recall']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.visbert import VisualBERT\n",
    "import torch \n",
    "from transformers import BertTokenizer, VisualBertModel\n",
    "\n",
    "model = VisualBERT()\n",
    "tokenizer_name = 'google-bert/bert-base-uncased'\n",
    "swin_model = \"microsoft/swinv2-base-patch4-window12-192-22k\"\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-bert/bert-base-uncased\n",
      "None\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/None/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    399\u001b[0m         path_or_repo_id,\n\u001b[1;32m    400\u001b[0m         filename,\n\u001b[1;32m    401\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    402\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    403\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    404\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    405\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    406\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    407\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    408\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    409\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    410\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    412\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1403\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1402\u001b[0m     \u001b[39m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1403\u001b[0m     \u001b[39mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1405\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1262\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1263\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1264\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1265\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1266\u001b[0m         library_name\u001b[39m=\u001b[39;49mlibrary_name,\n\u001b[1;32m   1267\u001b[0m         library_version\u001b[39m=\u001b[39;49mlibrary_version,\n\u001b[1;32m   1268\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1269\u001b[0m     )\n\u001b[1;32m   1270\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1271\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 119\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1674\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1674\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1675\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1676\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1677\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1678\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1679\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1680\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1681\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1682\u001b[0m )\n\u001b[1;32m   1683\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:369\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 369\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    370\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    371\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    372\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    373\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    376\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:393\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    392\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 393\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    394\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    345\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[0;32m--> 352\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-6623f8b0-6b3b1e2468d3fc5939cd34c5;315584c0-4598-46ac-8d2a-fb1c6b106d3e)\n\nRepository Not Found for url: https://huggingface.co/None/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model,\n\u001b[1;32m      3\u001b[0m     args,\n\u001b[0;32m----> 4\u001b[0m     train_dataset \u001b[39m=\u001b[39m DatasetBuilder(DatasetBuilder(swin_model,tokenizer_name,\u001b[39m'\u001b[39;49m\u001b[39mtrain.jsonl\u001b[39;49m\u001b[39m'\u001b[39;49m)),\n\u001b[1;32m      5\u001b[0m     eval_dataset \u001b[39m=\u001b[39m DatasetBuilder(DatasetBuilder(swin_model,tokenizer_name,\u001b[39m'\u001b[39m\u001b[39mtest.jsonl\u001b[39m\u001b[39m'\u001b[39m)),\n\u001b[1;32m      6\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m      7\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m      8\u001b[0m )\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, model, tokenizer, json_file)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjson_file \u001b[39m=\u001b[39m json_file\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(tokenizer)\n\u001b[0;32m---> 17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(tokenizer)\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_processor \u001b[39m=\u001b[39m AutoImageProcessor\u001b[39m.\u001b[39mfrom_pretrained(model)\n\u001b[1;32m     19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviz_model \u001b[39m=\u001b[39m Swinv2Model\u001b[39m.\u001b[39mfrom_pretrained(model)\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2007\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtokenizer_file\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m vocab_files:\n\u001b[1;32m   2005\u001b[0m     \u001b[39m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m     fast_tokenizer_file \u001b[39m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 2007\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   2008\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2009\u001b[0m         TOKENIZER_CONFIG_FILE,\n\u001b[1;32m   2010\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2011\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   2012\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   2013\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   2014\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2015\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2016\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2017\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   2018\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   2019\u001b[0m         _raise_exceptions_for_gated_repo\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   2020\u001b[0m         _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   2021\u001b[0m         _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   2022\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   2023\u001b[0m     )\n\u001b[1;32m   2024\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2025\u001b[0m     \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    417\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to have access to it at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 421\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`token=<your_token>`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39mexcept\u001b[39;00m RevisionNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    428\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    429\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor this model name. Check the model page at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available revisions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = DatasetBuilder(DatasetBuilder(swin_model,tokenizer_name,'train.jsonl')),\n",
    "    eval_dataset = DatasetBuilder(DatasetBuilder(swin_model,tokenizer_name,'test.jsonl')),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/tmp/ipykernel_45210/3443408764.py:44: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n",
      "  acc_metric = load_metric('accuracy')\n",
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-bert/bert-base-uncased\n",
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/experiments\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './tokenizer/'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    399\u001b[0m         path_or_repo_id,\n\u001b[1;32m    400\u001b[0m         filename,\n\u001b[1;32m    401\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    402\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    403\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    404\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    405\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    406\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    407\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    408\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    409\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    410\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    412\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:111\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 111\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    113\u001b[0m \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:159\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    160\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './tokenizer/'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m#visual_model.forward(dataset[0])\u001b[39;00m\n\u001b[1;32m     72\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(visual_model\u001b[39m.\u001b[39mparameters(), )\n\u001b[1;32m     74\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     75\u001b[0m     visual_model,\n\u001b[1;32m     76\u001b[0m     args,\n\u001b[0;32m---> 77\u001b[0m     train_dataset \u001b[39m=\u001b[39m DatasetBuilder(swin_model,tokenizer,json_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain.jsonl\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m     78\u001b[0m     eval_dataset \u001b[39m=\u001b[39m DatasetBuilder(swin_model,tokenizer,json_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest.jsonl\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     79\u001b[0m     \u001b[39m#tokenizer=BertTokenizer.from_pretrained(\"./tokenizer/\", local_files_only=True),\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     84\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     85\u001b[0m \u001b[39m#for i in range(1):\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m##    for j in dataset.get_dataset():\u001b[39;00m\n\u001b[1;32m     87\u001b[0m  \u001b[39m#       optimizer.zero_grad()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m  \u001b[39m#       optimizer.step()\u001b[39;00m\n\u001b[1;32m     91\u001b[0m  \u001b[39m#       print(loss)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/experiments/data_setup/databuilder.py:19\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, model, tokenizer, json_file)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjson_file \u001b[39m=\u001b[39m json_file\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(os\u001b[39m.\u001b[39mgetcwd())\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39m./tokenizer/\u001b[39;49m\u001b[39m\"\u001b[39;49m, local_files_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_processor \u001b[39m=\u001b[39m AutoImageProcessor\u001b[39m.\u001b[39mfrom_pretrained(model)\n\u001b[1;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviz_model \u001b[39m=\u001b[39m Swinv2Model\u001b[39m.\u001b[39mfrom_pretrained(model)\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2007\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtokenizer_file\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m vocab_files:\n\u001b[1;32m   2005\u001b[0m     \u001b[39m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m     fast_tokenizer_file \u001b[39m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 2007\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   2008\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2009\u001b[0m         TOKENIZER_CONFIG_FILE,\n\u001b[1;32m   2010\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2011\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   2012\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   2013\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   2014\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2015\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2016\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2017\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   2018\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   2019\u001b[0m         _raise_exceptions_for_gated_repo\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   2020\u001b[0m         _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   2021\u001b[0m         _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   2022\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   2023\u001b[0m     )\n\u001b[1;32m   2024\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2025\u001b[0m     \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/transformers/utils/hub.py:462\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThere was a specific connection error when trying to load \u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m \u001b[39mexcept\u001b[39;00m HFValidationError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 462\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    463\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncorrect path_or_model_id: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[39mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './tokenizer/'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "from data_setup.databuilder import DatasetBuilder\n",
    "\n",
    "import torch \n",
    "from transformers import BertTokenizer, VisualBertModel\n",
    "from transformers import Swinv2Model,AutoImageProcessor\n",
    "import os \n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from model.visbert import VisualBERT\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datasets import list_metrics, load_metric\n",
    "from transformers import TrainingArguments, Trainer\n",
    "batch_size = 24\n",
    "seq_len = 50\n",
    "\n",
    "metric_name = \"auroc\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"model-checkpoint\",\n",
    "    seed = 110, \n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs= 100,\n",
    "    weight_decay=0.05,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    eval_steps = 50,\n",
    "    save_steps = 500,\n",
    "    fp16 = False,\n",
    "    gradient_accumulation_steps = 2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#metrics_list = list_metrics()\n",
    "#print(metrics_list)\n",
    "\n",
    "\n",
    "acc_metric = load_metric('accuracy')\n",
    "f1_metric = load_metric('f1')\n",
    "precision_metric = load_metric('precision')\n",
    "recall_metric = load_metric('recall')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)\n",
    "    auc_score = roc_auc_score(labels, predictions)\n",
    "    return {\"accuracy\": acc['accuracy'], \"auroc\": auc_score,'f1':f1['f1'],'precision':precision['precision'],'recall':recall['recall']}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = 'google-bert/bert-base-uncased'\n",
    "model = \"microsoft/swinv2-base-patch4-window12-192-22k\"\n",
    "#dataset = DatasetBuilder(model,tokenizer,'train.jsonl')\n",
    "\n",
    "\n",
    "\n",
    "visual_model = VisualBERT()\n",
    "swin_model = \"microsoft/swinv2-base-patch4-window12-192-22k\"\n",
    "#visual_model.forward(dataset[0])\n",
    "\n",
    "optimizer = torch.optim.Adam(visual_model.parameters(), )\n",
    "\n",
    "trainer = Trainer(\n",
    "    visual_model,\n",
    "    args,\n",
    "    train_dataset = DatasetBuilder(swin_model,tokenizer,json_file='train.jsonl'),\n",
    "    eval_dataset = DatasetBuilder(swin_model,tokenizer,json_file='test.jsonl'),\n",
    "    #tokenizer=BertTokenizer.from_pretrained(\"./tokenizer/\", local_files_only=True),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "#for i in range(1):\n",
    "##    for j in dataset.get_dataset():\n",
    " #       optimizer.zero_grad()\n",
    "  #      loss, _ = visual_model(j)\n",
    " #       loss.backward()\n",
    " #       optimizer.step()\n",
    " #       print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import jsonlines\n",
    "\n",
    "parent_directory = os.path.dirname(os.getcwd())\n",
    "data_directory = os.path.join(parent_directory,'data')\n",
    "\n",
    "train_dataset = pd.DataFrame(columns=['id','img','label','text'])\n",
    "\n",
    "# Open the JSONL file in read mode using jsonlines\n",
    "with jsonlines.open(os.path.join(data_directory,'train.jsonl')) as reader:\n",
    "    # Iterate over each line in the file\n",
    "    for data in reader:\n",
    "        # Process the data as needed\n",
    "        data_df = pd.DataFrame([data])\n",
    "        train_dataset = pd.concat([train_dataset,data_df]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Swinv2Config, Swinv2Model\n",
    "\n",
    "# Initializing a Swinv2 microsoft/swinv2-tiny-patch4-window8-256 style configuration\n",
    "configuration = Swinv2Config()\n",
    "\n",
    "# Initializing a model (with random weights) from the microsoft/swinv2-tiny-patch4-window8-256 style configuration\n",
    "model = Swinv2Model(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Swinv2Config {\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"depths\": [\n",
       "    2,\n",
       "    2,\n",
       "    6,\n",
       "    2\n",
       "  ],\n",
       "  \"drop_path_rate\": 0.1,\n",
       "  \"embed_dim\": 96,\n",
       "  \"encoder_stride\": 32,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"mlp_ratio\": 4.0,\n",
       "  \"model_type\": \"swinv2\",\n",
       "  \"num_channels\": 3,\n",
       "  \"num_heads\": [\n",
       "    3,\n",
       "    6,\n",
       "    12,\n",
       "    24\n",
       "  ],\n",
       "  \"num_layers\": 4,\n",
       "  \"out_features\": [\n",
       "    \"stage4\"\n",
       "  ],\n",
       "  \"out_indices\": [\n",
       "    4\n",
       "  ],\n",
       "  \"patch_size\": 4,\n",
       "  \"pretrained_window_sizes\": [\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0\n",
       "  ],\n",
       "  \"qkv_bias\": true,\n",
       "  \"stage_names\": [\n",
       "    \"stem\",\n",
       "    \"stage1\",\n",
       "    \"stage2\",\n",
       "    \"stage3\",\n",
       "    \"stage4\"\n",
       "  ],\n",
       "  \"transformers_version\": \"4.39.3\",\n",
       "  \"use_absolute_embeddings\": false,\n",
       "  \"window_size\": 7\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os \n",
    "\n",
    "image = Image.open(os.path.join(data_directory,train_dataset.loc[0,'img']))\n",
    "image_array = np.array(image)\n",
    "tensor = torch.tensor(image_array)\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")\n",
    "viz_model = Swinv2Model.from_pretrained(\"microsoft/swinv2-tiny-patch4-window16-256\")\n",
    "\n",
    "inputs = image_processor(tensor, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = viz_model(**inputs)\n",
    "\n",
    "visual_embeds = outputs.last_hidden_state\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "print(visual_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 36, 1024])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os \n",
    "\n",
    "image = Image.open(os.path.join(data_directory,train_dataset.loc[0,'img']))\n",
    "image_array = np.array(image)\n",
    "tensor = torch.tensor(image_array)\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")\n",
    "viz_model = Swinv2Model.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")\n",
    "\n",
    "inputs = image_processor(tensor, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = viz_model(**inputs)\n",
    "\n",
    "visual_embeds = outputs.last_hidden_state\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "print(visual_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 1024])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, Swinv2ForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "\n",
    "image = Image.open(os.path.join(data_directory,train_dataset.loc[0,'img']))\n",
    "image_array = np.array(image)\n",
    "tensor = torch.tensor(image_array)\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "viz_model =Swinv2Model.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")\n",
    "\n",
    "inputs = image_processor(tensor, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = viz_model(**inputs)\n",
    "\n",
    "visual_embeds = outputs.last_hidden_state\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "print(visual_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<class 'transformers.models.swinv2.configuration_swinv2.Swinv2Config'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m swin_model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmicrosoft/swinv2-base-patch4-window12-192-22k\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m swin_model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(swin_model_name)\n\u001b[0;32m----> 9\u001b[0m swin_tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(swin_model_name)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Load the VisualBERT model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m visualbert_model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muclanlp/visualbert-vqa-coco-pre\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:853\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m model_type \u001b[39m=\u001b[39m config_class_to_model_type(\u001b[39mtype\u001b[39m(config)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    852\u001b[0m \u001b[39mif\u001b[39;00m model_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 853\u001b[0m     tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39;49m(config)]\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    855\u001b[0m         \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:742\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    740\u001b[0m         model_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_mapping[mtype]\n\u001b[1;32m    741\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_attr_from_module(mtype, model_name)\n\u001b[0;32m--> 742\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: <class 'transformers.models.swinv2.configuration_swinv2.Swinv2Config'>"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, Swinv2ForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "import os \n",
    "import pandas as pd \n",
    "import jsonlines\n",
    "import numpy as np \n",
    "\n",
    "parent_directory = os.path.dirname(os.getcwd())\n",
    "data_directory = os.path.join(parent_directory,'data')\n",
    "\n",
    "train_dataset = pd.DataFrame(columns=['id','img','label','text'])\n",
    "\n",
    "# Open the JSONL file in read mode using jsonlines\n",
    "with jsonlines.open(os.path.join(data_directory,'train.jsonl')) as reader:\n",
    "    # Iterate over each line in the file\n",
    "    for data in reader:\n",
    "        # Process the data as needed\n",
    "        data_df = pd.DataFrame([data])\n",
    "        train_dataset = pd.concat([train_dataset,data_df]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "image = Image.open(os.path.join(data_directory,train_dataset.loc[0,'img']))\n",
    "image_array = np.array(image)\n",
    "tensor = torch.tensor(image_array)\n",
    "\n",
    "\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "feature_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "inputs = feature_extractor(images=tensor, return_tensors=\"pt\")\n",
    "outputs = feature_model(**inputs)#.to('cuda'))\n",
    "\n",
    "visual_embeds = outputs['last_hidden_state']\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "print(visual_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, VisualBertModel, TrainingArguments, Trainer, VisualBertConfig\n",
    "configuration = VisualBertConfig.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre',\n",
    "                                                hidden_dropout_prob=0.3, attention_probs_dropout_prob=0.3)\n",
    "model = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre', config=configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisualBertModel(\n",
       "  (embeddings): VisualBertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (visual_token_type_embeddings): Embedding(2, 768)\n",
       "    (visual_position_embeddings): Embedding(512, 768)\n",
       "    (visual_projection): Linear(in_features=1024, out_features=768, bias=True)\n",
       "  )\n",
       "  (encoder): VisualBertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x VisualBertLayer(\n",
       "        (attention): VisualBertAttention(\n",
       "          (self): VisualBertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (output): VisualBertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): VisualBertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): VisualBertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): VisualBertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jramos/Documents/OMSCS/CS-7643 Deep Learning/Project/Hateful-Memes/workspace/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2+cu121'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
