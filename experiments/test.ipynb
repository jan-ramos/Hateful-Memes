{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "parent_directory = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "data_directory = os.path.join(parent_directory,'data')\n",
    "\n",
    "# Open the JSONL file in read mode using jsonlines\n",
    "with jsonlines.open(os.path.join(data_directory,'dev.jsonl')) as reader:\n",
    "    # Iterate over each line in the file\n",
    "    for data in reader:\n",
    "       \n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Transformer model from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VisualBert and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import BertTokenizer, VisualBertModel\n",
    "\n",
    "model = VisualBertModel.from_pretrained('uclanlp/visualbert-vqa-coco-pre')\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Swinv2Model for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Swinv2Model,AutoImageProcessor\n",
    "\n",
    "viz_model = Swinv2Model.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")#.from_pretrained('yusx-swapp/ofm-swinv2-base-patch4-window7-cifar100')\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")#.from_pretrained('yusx-swapp/ofm-swinv2-base-patch4-window7-cifar100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create DataFrame from train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import jsonlines\n",
    "\n",
    "parent_directory = os.path.dirname(os.getcwd())\n",
    "data_directory = os.path.join(parent_directory,'data')\n",
    "\n",
    "train_dataset = pd.DataFrame(columns=['id','img','label','text'])\n",
    "\n",
    "# Open the JSONL file in read mode using jsonlines\n",
    "with jsonlines.open(os.path.join(data_directory,'train.jsonl')) as reader:\n",
    "    # Iterate over each line in the file\n",
    "    for data in reader:\n",
    "        # Process the data as needed\n",
    "        data_df = pd.DataFrame([data])\n",
    "        train_dataset = pd.concat([train_dataset,data_df]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swinv2 vs. ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os \n",
    "\n",
    "image = Image.open(os.path.join(data_directory,train_dataset.loc[0,'img']))\n",
    "image_array = np.array(image)\n",
    "tensor = torch.tensor(image_array)\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")\n",
    "viz_model = Swinv2Model.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")\n",
    "\n",
    "inputs = image_processor(tensor, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = viz_model(**inputs)\n",
    "\n",
    "visual_embeds = outputs.last_hidden_state\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "print(visual_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "from PIL import Image\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "feature_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "inputs = feature_extractor(images=tensor, return_tensors=\"pt\")\n",
    "outputs = feature_model(**inputs)#.to('cuda'))\n",
    "\n",
    "visual_embeds = outputs['last_hidden_state']\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "print(visual_embeds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create input dataset for VisualBERT by connecting all the pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = train_dataset.loc[0,'text']\n",
    "input_text_tokenized = tokenizer(input_text, return_tensors='pt', padding='max_length', max_length=512, truncation=True)\n",
    "input_text_tokenized.update(\n",
    "    {\n",
    "        \"visual_embeds\": visual_embeds,\n",
    "        \"visual_token_type_ids\": visual_token_type_ids,\n",
    "        \"visual_attention_mask\": visual_attention_mask\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import BertTokenizer, VisualBertModel\n",
    "\n",
    "model = VisualBertModel.from_pretrained('uclanlp/visualbert-vqa-coco-pre')\n",
    "#model = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\n",
    "output = model(**input_text_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of VisualBERT input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      1. Text input \n",
    "      2. Image input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Text input requires three pieces: input_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - input_ids: represent the token IDs of the input tokens after tokenization. Each token in the input text is converted into a numerical ID based on the tokenizer's vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - attention_mask: indicates which tokens should be attended to during processing. Binary mask where each position corresponds to a token in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - token_type_ids: represent the segment IDs for token types in the context of sequence pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Image input requires three pieces: visual_embeds, visual_token_type_ids, visual_attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - visual_embeds: represents the embeddings of visual features in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - visual_token_type_ids: represent the token type IDs for visual toekens. In multi-modal transformers distinguishes between tokens representing visual features and tokens representing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - vision_attention_mask: guide the attention mechanism for visual tokens. Specifies which tokens should be attended to during processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Probably dont need what's below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "from transformers import BertTokenizer, VisualBertModel\n",
    "from transformers import Swinv2Model,AutoImageProcessor\n",
    "import os \n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DatasetBuilder():\n",
    "    def __init__(self,model = None,tokenizer = None,json_file = None):\n",
    "        self.parent_directory = os.path.dirname(os.getcwd())\n",
    "        self.data_directory = os.path.join(self.parent_directory,'data')\n",
    "        self.json_file = json_file\n",
    "        print(tokenizer)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer)\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(model)\n",
    "        self.viz_model = Swinv2Model.from_pretrained(model)\n",
    "        self.dataset = self.create_dataframe()\n",
    "\n",
    "    def create_dataframe(self):\n",
    "        dataset = pd.DataFrame(columns=['id','img','label','text'])\n",
    "        with jsonlines.open(os.path.join(self.data_directory,self.json_file)) as reader:\n",
    "            for data in reader:\n",
    "                data_df = pd.DataFrame([data])\n",
    "                dataset = pd.concat([dataset,data_df]).reset_index(drop=True)\n",
    "\n",
    "        dataset['img'] = (self.data_directory+'/'+dataset['img']).apply(self.load_image)\n",
    "        return dataset\n",
    "    \n",
    "\n",
    "    def tokenize_data(self,value):\n",
    "        input = self.tokenizer(value['text'], return_tensors='pt', padding='max_length', max_length=512, truncation=True)\n",
    "        target = torch.tensor(value['label']).type(torch.int64)\n",
    "        \n",
    "        \n",
    "        image = self.image_processor(value['img'], return_tensors=\"pt\")\n",
    "            \n",
    "            \n",
    "        outputs = self.viz_model(**image)\n",
    "\n",
    "        visual_embeds = outputs.last_hidden_state\n",
    "\n",
    "       # except:\n",
    "       #     visual_embeds = np.zeros(shape=(197, 768), dtype=float)\n",
    "\n",
    "            \n",
    "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "        input.update(\n",
    "            {\n",
    "                \"visual_embeds\": visual_embeds,\n",
    "                \"visual_token_type_ids\": visual_token_type_ids,\n",
    "                \"visual_attention_mask\": visual_attention_mask,\n",
    "                \"label\":target\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return input\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        return self.dataset\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset.loc[index])\n",
    "        \n",
    "        for k in inputs.keys():\n",
    "            print(k, inputs[k].shape, inputs[k].dtype)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def load_image(self,filepath):\n",
    "        image = Image.open(filepath)\n",
    "        image_array = np.array(image)\n",
    "        return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "batch_size = 24\n",
    "seq_len = 50\n",
    "\n",
    "metric_name = \"auroc\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"model-checkpoint\",\n",
    "    seed = 110, \n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs= 100,\n",
    "    weight_decay=0.05,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    eval_steps = 50,\n",
    "    save_steps = 500,\n",
    "    fp16 = False,\n",
    "    gradient_accumulation_steps = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datasets import list_metrics, load_metric\n",
    "metrics_list = list_metrics()\n",
    "print(metrics_list)\n",
    "\n",
    "\n",
    "acc_metric = load_metric('accuracy')\n",
    "f1_metric = load_metric('f1')\n",
    "precision_metric = load_metric('precision')\n",
    "recall_metric = load_metric('recall')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)\n",
    "    auc_score = roc_auc_score(labels, predictions)\n",
    "    return {\"accuracy\": acc['accuracy'], \"auroc\": auc_score,'f1':f1['f1'],'precision':precision['precision'],'recall':recall['recall']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.visbert import VisualBERT\n",
    "import torch \n",
    "from transformers import BertTokenizer, VisualBertModel\n",
    "\n",
    "model = VisualBERT()\n",
    "tokenizer_name = 'google-bert/bert-base-uncased'\n",
    "swin_model = \"microsoft/swinv2-base-patch4-window12-192-22k\"\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = DatasetBuilder(DatasetBuilder(swin_model,tokenizer_name,'train.jsonl')),\n",
    "    eval_dataset = DatasetBuilder(DatasetBuilder(swin_model,tokenizer_name,'test.jsonl')),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
