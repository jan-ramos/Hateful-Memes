{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "parent_directory = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "data_directory = os.path.join(parent_directory,'data')\n",
    "\n",
    "# Open the JSONL file in read mode using jsonlines\n",
    "with jsonlines.open(os.path.join(data_directory,'dev.jsonl')) as reader:\n",
    "    # Iterate over each line in the file\n",
    "    for data in reader:\n",
    "       \n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Transformer model from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VisualBert and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import BertTokenizer, VisualBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisualBertModel.from_pretrained('uclanlp/visualbert-vqa-coco-pre')\n",
    "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Swinv2Model for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Swinv2Model,AutoImageProcessor\n",
    "\n",
    "viz_model = Swinv2Model.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")#.from_pretrained('yusx-swapp/ofm-swinv2-base-patch4-window7-cifar100')\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")#.from_pretrained('yusx-swapp/ofm-swinv2-base-patch4-window7-cifar100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create DataFrame from train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "parent_directory = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "data_directory = os.path.join(parent_directory,'data')\n",
    "\n",
    "train_dataset = pd.DataFrame(columns=['id','img','label','text'])\n",
    "\n",
    "# Open the JSONL file in read mode using jsonlines\n",
    "with jsonlines.open(os.path.join(data_directory,'train.jsonl')) as reader:\n",
    "    # Iterate over each line in the file\n",
    "    for data in reader:\n",
    "        # Process the data as needed\n",
    "        data_df = pd.DataFrame([data])\n",
    "        train_dataset = pd.concat([train_dataset,data_df]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swinv2 vs. ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open(os.path.join(data_directory,train_dataset.loc[0,'img']))\n",
    "image_array = np.array(image)\n",
    "tensor = torch.tensor(image_array)\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")\n",
    "viz_model = Swinv2Model.from_pretrained(\"microsoft/swinv2-base-patch4-window12-192-22k\")\n",
    "\n",
    "inputs = image_processor(tensor, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = viz_model(**inputs)\n",
    "\n",
    "visual_embeds = outputs.last_hidden_state\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "from PIL import Image\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "feature_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "inputs = feature_extractor(images=tensor, return_tensors=\"pt\")\n",
    "outputs = feature_model(**inputs)#.to('cuda'))\n",
    "\n",
    "visual_embeds = outputs['last_hidden_state']\n",
    "visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create input dataset for VisualBERT by connecting all the pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.loc[0,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = train_dataset.loc[0,'text']\n",
    "input_text_tokenized = tokenizer(input_text, return_tensors='pt', padding='max_length', max_length=512, truncation=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_tokenized.update(\n",
    "    {\n",
    "        \"visual_embeds\": visual_embeds,\n",
    "        \"visual_token_type_ids\": visual_token_type_ids,\n",
    "        \"visual_attention_mask\": visual_attention_mask,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**input_text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand(36,1024)\n",
    "t2 = torch.rand(2048,768)\n",
    "\n",
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "\n",
    "print(torch.matmul(t1,t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_tokenized['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
